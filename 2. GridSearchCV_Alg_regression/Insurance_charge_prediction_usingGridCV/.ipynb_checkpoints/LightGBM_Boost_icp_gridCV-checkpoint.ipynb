{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8ede9d-412d-463d-a6c0-1ff17d7612ad",
   "metadata": {},
   "source": [
    "# LightGBM (boosting) - Regressor Algorithm - insurance_charge_prediction\n",
    "# using Grid Search CV \n",
    "## MODEL CREATION PHASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7ff60-fe0e-4ec8-9dfe-5ea9b658ee13",
   "metadata": {},
   "source": [
    "## read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f512fccb-e220-4abb-8870-abea1253083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>50</td>\n",
       "      <td>male</td>\n",
       "      <td>30.970</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>10600.54830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>2205.98080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>36.850</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>1629.83350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>21</td>\n",
       "      <td>female</td>\n",
       "      <td>25.800</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>2007.94500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>61</td>\n",
       "      <td>female</td>\n",
       "      <td>29.070</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>29141.36030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     sex     bmi  children smoker      charges\n",
       "0      19  female  27.900         0    yes  16884.92400\n",
       "1      18    male  33.770         1     no   1725.55230\n",
       "2      28    male  33.000         3     no   4449.46200\n",
       "3      33    male  22.705         0     no  21984.47061\n",
       "4      32    male  28.880         0     no   3866.85520\n",
       "...   ...     ...     ...       ...    ...          ...\n",
       "1333   50    male  30.970         3     no  10600.54830\n",
       "1334   18  female  31.920         0     no   2205.98080\n",
       "1335   18  female  36.850         0     no   1629.83350\n",
       "1336   21  female  25.800         0     no   2007.94500\n",
       "1337   61  female  29.070         0    yes  29141.36030\n",
       "\n",
       "[1338 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "dataset = pd.read_csv(\"insurance_pre.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b114e84-f389-4328-a61e-5d0f8ff7faf7",
   "metadata": {},
   "source": [
    "## convert categorical data into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3999b5-14b1-4acc-a477-d6038c4840d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>charges</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>smoker_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>50</td>\n",
       "      <td>30.970</td>\n",
       "      <td>3</td>\n",
       "      <td>10600.54830</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>18</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>2205.98080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>18</td>\n",
       "      <td>36.850</td>\n",
       "      <td>0</td>\n",
       "      <td>1629.83350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>21</td>\n",
       "      <td>25.800</td>\n",
       "      <td>0</td>\n",
       "      <td>2007.94500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>61</td>\n",
       "      <td>29.070</td>\n",
       "      <td>0</td>\n",
       "      <td>29141.36030</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi  children      charges  sex_male  smoker_yes\n",
       "0      19  27.900         0  16884.92400         0           1\n",
       "1      18  33.770         1   1725.55230         1           0\n",
       "2      28  33.000         3   4449.46200         1           0\n",
       "3      33  22.705         0  21984.47061         1           0\n",
       "4      32  28.880         0   3866.85520         1           0\n",
       "...   ...     ...       ...          ...       ...         ...\n",
       "1333   50  30.970         3  10600.54830         1           0\n",
       "1334   18  31.920         0   2205.98080         0           0\n",
       "1335   18  36.850         0   1629.83350         0           0\n",
       "1336   21  25.800         0   2007.94500         0           0\n",
       "1337   61  29.070         0  29141.36030         0           1\n",
       "\n",
       "[1338 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.get_dummies(dataset,drop_first=True,dtype=int)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad5f34-75eb-4d02-9c52-86281eb3d1b0",
   "metadata": {},
   "source": [
    "## split Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e353955-35ca-4d22-ad86-ca7687a4246d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'bmi', 'children', 'charges', 'sex_male', 'smoker_yes'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62e67956-9f8d-4941-bfda-433b2c622f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>smoker_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>50</td>\n",
       "      <td>30.970</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>18</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>18</td>\n",
       "      <td>36.850</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>21</td>\n",
       "      <td>25.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>61</td>\n",
       "      <td>29.070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi  children  sex_male  smoker_yes\n",
       "0      19  27.900         0         0           1\n",
       "1      18  33.770         1         1           0\n",
       "2      28  33.000         3         1           0\n",
       "3      33  22.705         0         1           0\n",
       "4      32  28.880         0         1           0\n",
       "...   ...     ...       ...       ...         ...\n",
       "1333   50  30.970         3         1           0\n",
       "1334   18  31.920         0         0           0\n",
       "1335   18  36.850         0         0           0\n",
       "1336   21  25.800         0         0           0\n",
       "1337   61  29.070         0         0           1\n",
       "\n",
       "[1338 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent = dataset[['age', 'bmi', 'children',  'sex_male', 'smoker_yes' ]]\n",
    "independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2f27cf-9541-48d7-9545-8c287c2220b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>10600.54830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>2205.98080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>1629.83350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>2007.94500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>29141.36030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          charges\n",
       "0     16884.92400\n",
       "1      1725.55230\n",
       "2      4449.46200\n",
       "3     21984.47061\n",
       "4      3866.85520\n",
       "...           ...\n",
       "1333  10600.54830\n",
       "1334   2205.98080\n",
       "1335   1629.83350\n",
       "1336   2007.94500\n",
       "1337  29141.36030\n",
       "\n",
       "[1338 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependent = dataset[['charges']]\n",
    "dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360706b-2127-484c-9d87-8defbd5320aa",
   "metadata": {},
   "source": [
    "## split train and test set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d84e048-ae08-4821-bed7-1668517d404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(independent, dependent, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c86a7cc-fd07-4417-9dae-4763607cb5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>smoker_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>52</td>\n",
       "      <td>30.200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>47</td>\n",
       "      <td>29.370</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>48</td>\n",
       "      <td>40.565</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>61</td>\n",
       "      <td>38.380</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>51</td>\n",
       "      <td>18.050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>62</td>\n",
       "      <td>30.495</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>41</td>\n",
       "      <td>28.405</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>57</td>\n",
       "      <td>40.280</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>30</td>\n",
       "      <td>39.050</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>46</td>\n",
       "      <td>24.795</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi  children  sex_male  smoker_yes\n",
       "578    52  30.200         1         1           0\n",
       "610    47  29.370         1         0           0\n",
       "569    48  40.565         2         1           1\n",
       "1034   61  38.380         0         1           0\n",
       "198    51  18.050         0         0           0\n",
       "...   ...     ...       ...       ...         ...\n",
       "1084   62  30.495         2         0           0\n",
       "726    41  28.405         1         1           0\n",
       "1132   57  40.280         0         1           0\n",
       "725    30  39.050         3         0           1\n",
       "963    46  24.795         3         1           0\n",
       "\n",
       "[268 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b9dfad-e3bb-49e3-8fe6-ae4e3def6654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>9724.53000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>8547.69130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>45702.02235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>12950.07120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>9644.25250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>15019.76005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>6664.68595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>20709.02034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>40932.42950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>9500.57305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          charges\n",
       "578    9724.53000\n",
       "610    8547.69130\n",
       "569   45702.02235\n",
       "1034  12950.07120\n",
       "198    9644.25250\n",
       "...           ...\n",
       "1084  15019.76005\n",
       "726    6664.68595\n",
       "1132  20709.02034\n",
       "725   40932.42950\n",
       "963    9500.57305\n",
       "\n",
       "[268 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ab198e-cf89-49c9-95dc-35f9fe3e5ca1",
   "metadata": {},
   "source": [
    "## model creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a1cedaa-a16c-48b2-ab8a-3a6c126f6ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in e:\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in e:\\anaconda3\\lib\\site-packages (from lightgbm) (2.1.3)\n",
      "Requirement already satisfied: scipy in e:\\anaconda3\\lib\\site-packages (from lightgbm) (1.15.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5d9d5c-8cd4-43df-8332-83b0943431be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b45dc82-aa8b-42db-b8f2-4e3a38afd852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 313\n",
      "[LightGBM] [Info] Number of data points in the train set: 1070, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 13201.182046\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LGBMRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [1.0, 0.5, 0.1, 0.01, 0.03, 0.05],\n",
       "                         &#x27;max_depth&#x27;: [-1, 3, 4, 5, 6],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 150, 200],\n",
       "                         &#x27;num_leaves&#x27;: [31, 8, 16, 32, 64]},\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=LGBMRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [1.0, 0.5, 0.1, 0.01, 0.03, 0.05],\n",
       "                         &#x27;max_depth&#x27;: [-1, 3, 4, 5, 6],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 150, 200],\n",
       "                         &#x27;num_leaves&#x27;: [31, 8, 16, 32, 64]},\n",
       "             verbose=3)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: LGBMRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(learning_rate=0.03, max_depth=3, n_estimators=150)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LGBMRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(learning_rate=0.03, max_depth=3, n_estimators=150)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=LGBMRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [1.0, 0.5, 0.1, 0.01, 0.03, 0.05],\n",
       "                         'max_depth': [-1, 3, 4, 5, 6],\n",
       "                         'n_estimators': [50, 100, 150, 200],\n",
       "                         'num_leaves': [31, 8, 16, 32, 64]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "classlightgbm.LGBMRegressor(*, boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=None, importance_type='split', **kwargs)\n",
    "-> max_depth (int, optional (default=-1)) – Maximum tree depth for base learners, <=0 means no limit. If setting this to a positive value, consider also changing num_leaves to <= 2^max_depth.\n",
    "\n",
    "\n",
    "tuning - hyper parameters :  n_estimators, max_depth, learning_rate, num_leaves \n",
    "\n",
    "\"\"\"\n",
    "# regressor = lgb.LGBMRegressor(boosting_type='gbdt', num_leaves=16, max_depth=4, learning_rate=0.03, n_estimators=200)\n",
    "# regressor.fit(x_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "  \"n_estimators\":[50,100,150,200],\n",
    "    \"learning_rate\":[1.0,0.5,0.1,0.01,0.03,0.05],\n",
    "   \"max_depth\":[-1,3,4,5,6],\n",
    "    \"num_leaves\": [31,8,16,32,64]\n",
    "}\n",
    "\n",
    "model=lgb.LGBMRegressor()\n",
    "\n",
    "grid_model = GridSearchCV(model, param_grid, refit=True, verbose=3, n_jobs=-1)\n",
    "\n",
    "grid_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d52dc9-38d5-4bc4-9f9f-f921f9ae8723",
   "metadata": {},
   "source": [
    "## Grid Search CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54dd9870-405e-4692-bc75-3120c6c79643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.35468664, 0.07462583, 0.12328515, 0.18462462, 0.21391449,\n",
       "        0.36254859, 0.11376991, 0.21573257, 0.45642433, 0.49284492,\n",
       "        0.81007605, 0.36303468, 0.4296247 , 0.73926101, 1.47158127,\n",
       "        0.93870301, 0.26550341, 0.36862354, 0.78580341, 1.26181197,\n",
       "        0.08753471, 0.14786139, 0.30710802, 0.12774982, 0.12947941,\n",
       "        0.23554173, 0.16347947, 0.26310039, 0.2339416 , 0.24724145,\n",
       "        0.35679836, 0.41036873, 0.62817068, 0.57536349, 0.39124708,\n",
       "        0.63065743, 0.38609719, 0.59735703, 0.47272367, 0.5250926 ,\n",
       "        0.18146949, 0.0910038 , 0.15702734, 0.56972995, 0.72683067,\n",
       "        0.25387626, 0.21249843, 0.41684103, 0.25318427, 0.30474048,\n",
       "        0.56656637, 0.24168344, 0.47926111, 0.3891942 , 0.33173661,\n",
       "        0.50345626, 0.43021126, 0.47147431, 0.45200996, 0.5010365 ,\n",
       "        0.25696564, 0.63145924, 0.14314919, 0.13922672, 0.19254923,\n",
       "        0.58556404, 0.31924357, 0.27810841, 0.35817366, 0.29086123,\n",
       "        0.65689731, 0.19581599, 0.36376023, 1.11626167, 0.48243237,\n",
       "        0.87299423, 0.23133454, 0.4868154 , 0.43453636, 0.47941608,\n",
       "        0.14441056, 0.0730979 , 0.10566649, 0.16308913, 0.1517983 ,\n",
       "        0.24956608, 0.11543183, 0.18574085, 0.28139701, 0.28472552,\n",
       "        0.43436971, 0.16288319, 0.27853746, 0.4060441 , 0.40548096,\n",
       "        0.54071608, 0.19559913, 0.38482609, 0.49334836, 0.51317134,\n",
       "        0.18891215, 0.08698235, 0.10768113, 0.17899899, 0.20583   ,\n",
       "        0.33041797, 0.11316533, 0.18217354, 0.32525239, 0.39015708,\n",
       "        0.45657077, 0.15774479, 0.26205807, 0.50637121, 0.56573715,\n",
       "        0.58617721, 0.21684637, 0.35955777, 0.64131122, 0.75710106,\n",
       "        0.08635149, 0.08488555, 0.09352498, 0.11404939, 0.09814677,\n",
       "        0.16403432, 0.15742064, 0.17457275, 0.1708313 , 0.15976348,\n",
       "        0.24274716, 0.21398454, 0.24150648, 0.24925427, 0.25409455,\n",
       "        0.31823487, 0.26797366, 0.29893012, 0.3008338 , 0.31628385,\n",
       "        0.10444822, 0.0853786 , 0.10118957, 0.12338557, 0.10558453,\n",
       "        0.20073547, 0.13868289, 0.53204474, 0.68025589, 0.19236107,\n",
       "        0.28545461, 0.18108888, 0.28242168, 0.30117187, 0.26680384,\n",
       "        0.36027336, 0.22216163, 0.38391886, 0.36476417, 0.41000824,\n",
       "        0.12612314, 0.07232928, 0.10468788, 0.12019138, 0.14109592,\n",
       "        0.22669401, 0.12111359, 0.20548377, 0.22524495, 0.25178094,\n",
       "        0.33404288, 0.16505456, 0.28583398, 0.33511124, 0.3442256 ,\n",
       "        0.42617822, 0.23702307, 0.37774396, 0.43973045, 0.46714182,\n",
       "        0.13718491, 0.07057657, 0.11729746, 0.14876895, 0.13583021,\n",
       "        0.25588789, 0.12998052, 0.19704471, 0.25559149, 0.28730664,\n",
       "        0.36372013, 0.16343656, 0.28547187, 0.35590625, 0.3696455 ,\n",
       "        0.48551016, 0.21210513, 0.36523867, 0.46570678, 0.47961712,\n",
       "        0.17762098, 0.07977419, 0.10664973, 0.17453957, 0.20500164,\n",
       "        0.30480266, 0.1201468 , 0.17661853, 0.34341898, 0.40791712,\n",
       "        0.43644438, 0.15512848, 0.27478867, 0.46710072, 0.60188498,\n",
       "        0.56233811, 0.22476025, 0.337009  , 0.63556628, 0.814467  ,\n",
       "        0.07443318, 0.08016348, 0.09426441, 0.098458  , 0.11063218,\n",
       "        0.18834848, 0.15738955, 0.16996927, 0.17287173, 0.17514734,\n",
       "        0.25007687, 0.18965893, 0.24213119, 0.22987471, 0.23507428,\n",
       "        0.33248444, 0.25929532, 0.30146117, 0.30734148, 0.31320662,\n",
       "        0.11088324, 0.07770448, 0.11041927, 0.13652959, 0.13148661,\n",
       "        0.22470474, 0.13230591, 0.19694257, 0.20830283, 0.1936615 ,\n",
       "        0.28110738, 0.17759528, 0.27617311, 0.28686194, 0.27890272,\n",
       "        0.35423265, 0.22693214, 0.35409741, 0.34696465, 0.39732594,\n",
       "        0.14352236, 0.07413912, 0.153549  , 0.13817973, 0.13593359,\n",
       "        0.23448629, 0.11199374, 0.19367394, 0.22913065, 0.23448949,\n",
       "        0.31622534, 0.17550864, 0.28281436, 0.3230916 , 0.32517128,\n",
       "        0.42379308, 0.22419081, 0.3618433 , 0.41143918, 0.40822124,\n",
       "        0.14712305, 0.08868876, 0.11273303, 0.15166588, 0.16055613,\n",
       "        0.24993382, 0.1239933 , 0.19337435, 0.26035533, 0.25522618,\n",
       "        0.36710629, 0.17377973, 0.37270966, 0.36994448, 0.36269765,\n",
       "        0.4570672 , 0.22141848, 0.3838954 , 0.43995056, 0.44935832,\n",
       "        0.17592664, 0.07650938, 0.11309919, 0.18952155, 0.20650692,\n",
       "        0.29470587, 0.12395186, 0.18829346, 0.3157485 , 0.36897063,\n",
       "        0.42804527, 0.1707356 , 0.28527708, 0.45573821, 0.53703718,\n",
       "        0.58602009, 0.23924823, 0.38372021, 0.80642252, 0.74789829,\n",
       "        0.09037209, 0.06496854, 0.11101999, 0.10783329, 0.10188923,\n",
       "        0.18582087, 0.11329455, 0.17223191, 0.1737009 , 0.18223524,\n",
       "        0.2506979 , 0.14292607, 0.2469759 , 0.25533433, 0.25660753,\n",
       "        0.32320514, 0.18216548, 0.32361631, 0.32024093, 0.32666125,\n",
       "        0.13162403, 0.07951422, 0.12782741, 0.12334447, 0.12538457,\n",
       "        0.22458243, 0.12033734, 0.23274436, 0.22315626, 0.22839193,\n",
       "        0.34890156, 0.17447839, 0.32596231, 0.33094225, 0.50460467,\n",
       "        0.58639483, 0.22306252, 0.42092261, 0.43842769, 0.43435001,\n",
       "        0.16234736, 0.07638087, 0.09625673, 0.14654026, 0.15061293,\n",
       "        0.27433801, 0.1245213 , 0.17552304, 0.28651905, 0.27645211,\n",
       "        0.41106071, 0.16225553, 0.255443  , 0.39617085, 0.43239856,\n",
       "        0.54006376, 0.21271076, 0.32558212, 0.52200732, 0.57333369,\n",
       "        0.16905851, 0.07380304, 0.10440331, 0.17559385, 0.18667722,\n",
       "        0.30865202, 0.13736119, 0.19848609, 0.33714395, 0.32534842,\n",
       "        0.47422481, 0.17551179, 0.27493267, 0.45494099, 0.47801781,\n",
       "        0.7337286 , 0.22633224, 0.35411191, 0.61443739, 0.64219484,\n",
       "        0.18351884, 0.10818734, 0.10313072, 0.18047037, 0.2025528 ,\n",
       "        0.3034245 , 0.10678215, 0.18646169, 0.31091981, 0.40556579,\n",
       "        0.44892445, 0.16170311, 0.25998545, 0.71494246, 1.05382462,\n",
       "        0.57518716, 0.1987958 , 0.34881787, 0.67739034, 0.74963293,\n",
       "        0.09096498, 0.06587696, 0.09183412, 0.09626193, 0.10617213,\n",
       "        0.1814724 , 0.10886025, 0.17919455, 0.17133594, 0.18622398,\n",
       "        0.26017051, 0.17359772, 0.23618722, 0.24128428, 0.26278205,\n",
       "        0.327811  , 0.25901904, 0.31875081, 0.32391167, 0.31516991,\n",
       "        0.12581306, 0.07257571, 0.12703614, 0.13904538, 0.1250463 ,\n",
       "        0.2151895 , 0.11977553, 0.21745238, 0.22906423, 0.22281775,\n",
       "        0.30343947, 0.17249103, 0.30891623, 0.32505674, 0.3114296 ,\n",
       "        0.39943633, 0.23379931, 0.38089271, 0.38616605, 0.38269882,\n",
       "        0.16045599, 0.07398009, 0.10151305, 0.15036516, 0.15143032,\n",
       "        0.28253746, 0.11730914, 0.1867867 , 0.27036557, 0.26947227,\n",
       "        0.35886779, 0.17372818, 0.28332558, 0.35929484, 0.36636653,\n",
       "        0.48235083, 0.22231894, 0.36298413, 0.45299358, 0.47450404,\n",
       "        0.19171939, 0.07197223, 0.10625443, 0.17424159, 0.1881227 ,\n",
       "        0.30876465, 0.11766276, 0.1838285 , 0.31379957, 0.31332316,\n",
       "        0.4092484 , 0.15895872, 0.27331505, 0.42680726, 0.43806677,\n",
       "        0.56283269, 0.20704641, 0.37515697, 0.53011174, 0.50883856,\n",
       "        0.17857909, 0.07512617, 0.10466924, 0.17095847, 0.19753661,\n",
       "        0.29650178, 0.11110458, 0.20988603, 0.32980323, 0.38460188,\n",
       "        0.44882326, 0.16002374, 0.2583672 , 0.46258039, 0.5808301 ,\n",
       "        0.58332825, 0.21837668, 0.33532848, 0.63820796, 0.7454217 ,\n",
       "        0.09639869, 0.07037911, 0.0999001 , 0.11618018, 0.11133499,\n",
       "        0.17447543, 0.12610168, 0.16358595, 0.16484218, 0.18369956,\n",
       "        0.23612056, 0.19080491, 0.25413208, 0.2377543 , 0.24809456,\n",
       "        0.31216745, 0.24076219, 0.29907632, 0.31579957, 0.32450233,\n",
       "        0.12987685, 0.0836328 , 0.13070388, 0.12386799, 0.13937302,\n",
       "        0.19463401, 0.12212181, 0.21450415, 0.20404568, 0.22485595,\n",
       "        0.30378842, 0.18117609, 0.28288326, 0.29582276, 0.28815455,\n",
       "        0.39304795, 0.23119698, 0.37013574, 0.37536669, 0.37901974,\n",
       "        0.14352527, 0.07210441, 0.10871058, 0.14430156, 0.14469314,\n",
       "        0.2539072 , 0.11976147, 0.19916425, 0.25526638, 0.27719579,\n",
       "        0.33479233, 0.17878623, 0.2883369 , 0.33535604, 0.35606985,\n",
       "        0.42877278, 0.22535677, 0.40644016, 0.47355466, 0.43947396,\n",
       "        0.18102865, 0.07372122, 0.10939665, 0.16871419, 0.16389933,\n",
       "        0.29789867, 0.11753325, 0.19987946, 0.28962154, 0.29353757,\n",
       "        0.38720512, 0.17041235, 0.27370191, 0.44473586, 0.39317617,\n",
       "        0.48831983, 0.23177948, 0.37734799, 0.47608738, 0.47955322]),\n",
       " 'std_fit_time': array([0.21718096, 0.01050932, 0.01403265, 0.02293247, 0.01170547,\n",
       "        0.0901776 , 0.01214199, 0.02330509, 0.09319546, 0.09957342,\n",
       "        0.19972146, 0.22866134, 0.14259173, 0.07327102, 0.7549711 ,\n",
       "        0.20035411, 0.03601688, 0.04146992, 0.08017927, 0.20116102,\n",
       "        0.0073292 , 0.10280457, 0.17099679, 0.02447868, 0.02345073,\n",
       "        0.0437695 , 0.01581431, 0.11187808, 0.05722508, 0.09128951,\n",
       "        0.09391344, 0.19471049, 0.23834267, 0.0751397 , 0.06297441,\n",
       "        0.25362002, 0.03813049, 0.14953217, 0.10910534, 0.27425316,\n",
       "        0.05968521, 0.00943779, 0.0588495 , 0.93073129, 0.89501661,\n",
       "        0.06260017, 0.03129326, 0.19092718, 0.04632017, 0.02294188,\n",
       "        0.1528573 , 0.04010391, 0.1767621 , 0.03230211, 0.05030966,\n",
       "        0.07365155, 0.14341813, 0.03921207, 0.05893559, 0.10848395,\n",
       "        0.13581251, 0.51377226, 0.02125598, 0.01554474, 0.05659341,\n",
       "        0.08944467, 0.16624111, 0.07437191, 0.08660866, 0.01848518,\n",
       "        0.22252257, 0.03074244, 0.04412509, 0.7645992 , 0.10424785,\n",
       "        0.4424849 , 0.06004107, 0.05566091, 0.01758872, 0.05220963,\n",
       "        0.01729305, 0.00591203, 0.00476647, 0.02229447, 0.01016277,\n",
       "        0.01278855, 0.00463644, 0.00641643, 0.0136394 , 0.01755514,\n",
       "        0.02988406, 0.01150296, 0.00615092, 0.03276237, 0.01736924,\n",
       "        0.02279444, 0.01268841, 0.0264111 , 0.01887054, 0.02594655,\n",
       "        0.02303844, 0.01546785, 0.00490986, 0.01419785, 0.00928182,\n",
       "        0.02702856, 0.01055699, 0.00548402, 0.01603101, 0.02127746,\n",
       "        0.03621774, 0.01899675, 0.00629508, 0.04110145, 0.02019418,\n",
       "        0.0163981 , 0.02513019, 0.01759755, 0.0371689 , 0.03365865,\n",
       "        0.01888058, 0.00199157, 0.00702783, 0.01750606, 0.00867062,\n",
       "        0.00237863, 0.01732578, 0.00975626, 0.01187857, 0.00606047,\n",
       "        0.00837174, 0.01294319, 0.0164729 , 0.0129242 , 0.03100635,\n",
       "        0.01466897, 0.01958376, 0.00853667, 0.01231731, 0.0167554 ,\n",
       "        0.00590124, 0.01241338, 0.00338998, 0.01485074, 0.00864526,\n",
       "        0.00730455, 0.03294701, 0.42120914, 0.41779192, 0.00950922,\n",
       "        0.0092055 , 0.01865835, 0.00891159, 0.03479349, 0.00551379,\n",
       "        0.01362885, 0.01582825, 0.02567442, 0.02031798, 0.03820441,\n",
       "        0.0118488 , 0.01046637, 0.00380894, 0.00683647, 0.01761407,\n",
       "        0.01090368, 0.00835251, 0.00917173, 0.00843923, 0.03344631,\n",
       "        0.03336237, 0.01856052, 0.01297943, 0.03757025, 0.02823948,\n",
       "        0.01905826, 0.01648812, 0.02203886, 0.04860599, 0.02009719,\n",
       "        0.00755549, 0.01284792, 0.00819586, 0.01707093, 0.00631695,\n",
       "        0.00813254, 0.01419264, 0.01808703, 0.02243453, 0.07117729,\n",
       "        0.01977248, 0.01238092, 0.01245459, 0.01133655, 0.04192604,\n",
       "        0.0290143 , 0.02094365, 0.01697456, 0.02873781, 0.02846723,\n",
       "        0.02572125, 0.00804173, 0.00801172, 0.00793406, 0.00942744,\n",
       "        0.0162656 , 0.01759304, 0.00432883, 0.04060945, 0.04357699,\n",
       "        0.02340986, 0.01373313, 0.02117853, 0.01589797, 0.03122453,\n",
       "        0.01824685, 0.03277232, 0.01666907, 0.01963844, 0.03748253,\n",
       "        0.00259296, 0.02458114, 0.00615365, 0.00433045, 0.01696206,\n",
       "        0.02292961, 0.01855432, 0.01422011, 0.00991214, 0.01752851,\n",
       "        0.01895799, 0.00870936, 0.01126352, 0.01341822, 0.00355411,\n",
       "        0.02996311, 0.01594123, 0.01110099, 0.02026307, 0.0040449 ,\n",
       "        0.01027773, 0.01088236, 0.00511868, 0.02318925, 0.02120239,\n",
       "        0.0294603 , 0.02281219, 0.00883115, 0.01703394, 0.00679558,\n",
       "        0.00629768, 0.02366724, 0.01790917, 0.01862275, 0.0176539 ,\n",
       "        0.01592053, 0.00970812, 0.00812502, 0.00840043, 0.00644375,\n",
       "        0.01724985, 0.00764723, 0.05028598, 0.00578541, 0.00436865,\n",
       "        0.00982003, 0.00453372, 0.01305092, 0.01715086, 0.02073703,\n",
       "        0.01114352, 0.01255233, 0.00709809, 0.00871845, 0.02261545,\n",
       "        0.0341539 , 0.02067068, 0.01223658, 0.01397653, 0.00876204,\n",
       "        0.011589  , 0.01960376, 0.00931891, 0.00306564, 0.01571385,\n",
       "        0.00939428, 0.00972528, 0.01004037, 0.00857812, 0.02082594,\n",
       "        0.01403969, 0.02017453, 0.08095286, 0.01934124, 0.03028226,\n",
       "        0.00593466, 0.02282555, 0.01582813, 0.01814603, 0.00737266,\n",
       "        0.01970276, 0.00663339, 0.01348725, 0.02634025, 0.01488402,\n",
       "        0.01874079, 0.017659  , 0.01207258, 0.02712081, 0.00838433,\n",
       "        0.03135445, 0.01577097, 0.0120401 , 0.02367134, 0.01843311,\n",
       "        0.01904026, 0.01820247, 0.0237695 , 0.20517545, 0.04048783,\n",
       "        0.01315964, 0.00363235, 0.00688261, 0.018637  , 0.01145085,\n",
       "        0.01818099, 0.00825351, 0.00948262, 0.00882814, 0.01280754,\n",
       "        0.00852376, 0.00871788, 0.01887863, 0.012836  , 0.01313682,\n",
       "        0.01166077, 0.01248468, 0.00984189, 0.00874009, 0.01468288,\n",
       "        0.01379079, 0.00803481, 0.01477288, 0.00767737, 0.00400845,\n",
       "        0.00773243, 0.00302255, 0.01072607, 0.01014812, 0.0170464 ,\n",
       "        0.02309833, 0.00587778, 0.02235531, 0.03000411, 0.13749337,\n",
       "        0.14575249, 0.01983916, 0.0198359 , 0.02334795, 0.01925612,\n",
       "        0.01916706, 0.00824661, 0.00129999, 0.00662922, 0.00988251,\n",
       "        0.0230459 , 0.01107439, 0.00681302, 0.01314579, 0.01465297,\n",
       "        0.01511055, 0.00667902, 0.02221967, 0.00981859, 0.02926935,\n",
       "        0.01009252, 0.00542733, 0.0288585 , 0.03558486, 0.04559616,\n",
       "        0.01282614, 0.00760606, 0.00380791, 0.00742881, 0.01541695,\n",
       "        0.01181342, 0.02923702, 0.01231516, 0.0164855 , 0.00858278,\n",
       "        0.02206657, 0.01295726, 0.01831062, 0.02305887, 0.01507158,\n",
       "        0.09190618, 0.02529059, 0.01472435, 0.02762612, 0.02779418,\n",
       "        0.03411213, 0.04104993, 0.00466447, 0.01464215, 0.01070209,\n",
       "        0.01414099, 0.00468849, 0.00730934, 0.02227958, 0.02521982,\n",
       "        0.0259993 , 0.00614936, 0.01436353, 0.19850242, 0.17170994,\n",
       "        0.0330463 , 0.02390203, 0.01884252, 0.03168625, 0.03221014,\n",
       "        0.01615132, 0.00592423, 0.00467129, 0.00259557, 0.01345095,\n",
       "        0.01070501, 0.01388084, 0.01393135, 0.0065964 , 0.01290212,\n",
       "        0.0269054 , 0.01017491, 0.00885558, 0.01219403, 0.01984231,\n",
       "        0.02511166, 0.0176948 , 0.00820467, 0.01184489, 0.00661502,\n",
       "        0.00730537, 0.00214541, 0.01519141, 0.01728536, 0.01008116,\n",
       "        0.00162567, 0.01174626, 0.00874833, 0.01290641, 0.00734793,\n",
       "        0.01375249, 0.01157553, 0.01357255, 0.0255832 , 0.01586416,\n",
       "        0.01346399, 0.0332892 , 0.0178337 , 0.01305234, 0.00961047,\n",
       "        0.01964942, 0.00135619, 0.0056392 , 0.00685751, 0.00854637,\n",
       "        0.02084269, 0.00739238, 0.01607461, 0.01586702, 0.00997004,\n",
       "        0.02556665, 0.01194119, 0.02335194, 0.01431269, 0.0241471 ,\n",
       "        0.06299885, 0.01227615, 0.01845114, 0.01642515, 0.0241232 ,\n",
       "        0.01330132, 0.00483156, 0.01175725, 0.01574052, 0.02514426,\n",
       "        0.01762758, 0.00910939, 0.01009241, 0.02345733, 0.0179272 ,\n",
       "        0.01930504, 0.00499117, 0.03028116, 0.01793189, 0.02254308,\n",
       "        0.04381598, 0.0107776 , 0.03295951, 0.02877901, 0.01624297,\n",
       "        0.03364047, 0.0127372 , 0.00471525, 0.00273334, 0.01244226,\n",
       "        0.02157272, 0.01185103, 0.03436205, 0.01768375, 0.0129157 ,\n",
       "        0.04090207, 0.01042003, 0.00702759, 0.01766076, 0.03999665,\n",
       "        0.01334243, 0.02392783, 0.02786566, 0.05117772, 0.02548795,\n",
       "        0.01626346, 0.01132208, 0.0043298 , 0.02412759, 0.01824727,\n",
       "        0.01903213, 0.00541211, 0.01177007, 0.00481132, 0.01240448,\n",
       "        0.01038826, 0.0173979 , 0.02126041, 0.00854604, 0.02021879,\n",
       "        0.01443579, 0.00848897, 0.00757602, 0.01788977, 0.02670472,\n",
       "        0.01577611, 0.01172646, 0.0154826 , 0.0049075 , 0.02125476,\n",
       "        0.00593087, 0.00858531, 0.01250444, 0.00623377, 0.01031302,\n",
       "        0.01583539, 0.01824751, 0.01195817, 0.01463508, 0.00801878,\n",
       "        0.04183476, 0.01471232, 0.01243444, 0.01656762, 0.02710733,\n",
       "        0.00702661, 0.00690658, 0.01066368, 0.00729537, 0.00145291,\n",
       "        0.01388447, 0.00514984, 0.00963382, 0.01983791, 0.02672573,\n",
       "        0.01377041, 0.01216218, 0.01885798, 0.01635359, 0.02087058,\n",
       "        0.02080771, 0.01848568, 0.11196626, 0.08064707, 0.02418734,\n",
       "        0.01379616, 0.00197778, 0.00593123, 0.00665561, 0.00954621,\n",
       "        0.01400291, 0.01138242, 0.01998816, 0.0285252 , 0.00983063,\n",
       "        0.01542362, 0.01482226, 0.00966773, 0.0529707 , 0.01393714,\n",
       "        0.02841189, 0.03015738, 0.01575798, 0.01776609, 0.04079624]),\n",
       " 'mean_score_time': array([0.01106954, 0.01279955, 0.0097034 , 0.00926752, 0.01045237,\n",
       "        0.01104975, 0.01289792, 0.01039877, 0.01069403, 0.01499195,\n",
       "        0.01462421, 0.0112741 , 0.01985722, 0.02121119, 0.01662173,\n",
       "        0.01969233, 0.01518931, 0.02148261, 0.0193347 , 0.01664872,\n",
       "        0.01111979, 0.01000395, 0.01888537, 0.01338577, 0.01702271,\n",
       "        0.01101189, 0.01842608, 0.0167439 , 0.01336722, 0.01413417,\n",
       "        0.01529675, 0.01365261, 0.03664436, 0.01765924, 0.01785836,\n",
       "        0.01712027, 0.0127593 , 0.02134738, 0.01484456, 0.01926274,\n",
       "        0.07280025, 0.0117218 , 0.02420821, 0.01145396, 0.01197081,\n",
       "        0.01208358, 0.01597624, 0.01421437, 0.01509929, 0.01669655,\n",
       "        0.01786785, 0.0141181 , 0.01758647, 0.01730905, 0.01321597,\n",
       "        0.01184411, 0.04725456, 0.01166134, 0.01344833, 0.02596416,\n",
       "        0.04136138, 0.01319547, 0.01429114, 0.01025867, 0.01204934,\n",
       "        0.01335087, 0.01576204, 0.01450748, 0.01183906, 0.0132237 ,\n",
       "        0.01250739, 0.01286936, 0.01302757, 0.0325563 , 0.014924  ,\n",
       "        0.02437558, 0.01090021, 0.01250663, 0.01314616, 0.01162157,\n",
       "        0.01543922, 0.01333437, 0.01409655, 0.01098928, 0.01050692,\n",
       "        0.01064482, 0.00967541, 0.01339192, 0.01282907, 0.01235986,\n",
       "        0.01261616, 0.01189404, 0.01413293, 0.01242447, 0.01246862,\n",
       "        0.01253753, 0.01602106, 0.01235251, 0.01277966, 0.01300263,\n",
       "        0.01186595, 0.01057243, 0.00955153, 0.01819129, 0.01203156,\n",
       "        0.0108923 , 0.00960412, 0.01004367, 0.01221919, 0.01073265,\n",
       "        0.01213808, 0.00990486, 0.01139731, 0.01294036, 0.01220055,\n",
       "        0.0126616 , 0.01050549, 0.01151185, 0.01305494, 0.01392541,\n",
       "        0.01120491, 0.01060677, 0.00993972, 0.01096234, 0.01119142,\n",
       "        0.01130247, 0.01168447, 0.01352692, 0.01369886, 0.01118193,\n",
       "        0.01152005, 0.01027775, 0.01232958, 0.01295071, 0.01124063,\n",
       "        0.01120858, 0.01169529, 0.01048045, 0.01121306, 0.01274467,\n",
       "        0.01234674, 0.01006675, 0.01187224, 0.01192579, 0.01453338,\n",
       "        0.01269755, 0.01270413, 0.02702999, 0.01331873, 0.01535897,\n",
       "        0.01164951, 0.01110644, 0.01309733, 0.0118371 , 0.01098094,\n",
       "        0.01269712, 0.01097679, 0.01204104, 0.0144228 , 0.01099639,\n",
       "        0.01033058, 0.01674747, 0.01018081, 0.0089788 , 0.01195817,\n",
       "        0.01163521, 0.01055775, 0.01093998, 0.01136479, 0.01163259,\n",
       "        0.01287732, 0.01236701, 0.01172166, 0.01426172, 0.01324697,\n",
       "        0.01202126, 0.01147451, 0.01210604, 0.01289859, 0.01154618,\n",
       "        0.01045709, 0.01074414, 0.01066232, 0.01044478, 0.00925608,\n",
       "        0.01111269, 0.0093473 , 0.01151476, 0.0120121 , 0.01280441,\n",
       "        0.01097612, 0.01198115, 0.01301384, 0.01135497, 0.01153731,\n",
       "        0.01215005, 0.01135707, 0.01148605, 0.01248493, 0.01188922,\n",
       "        0.0148417 , 0.00909004, 0.00929918, 0.01041536, 0.01023436,\n",
       "        0.01088719, 0.01409001, 0.01237154, 0.01055722, 0.01132665,\n",
       "        0.01238718, 0.00994887, 0.00989485, 0.01224341, 0.01241517,\n",
       "        0.01210122, 0.01049252, 0.01241198, 0.01502457, 0.01193762,\n",
       "        0.01018481, 0.01111259, 0.01036415, 0.01061292, 0.01110916,\n",
       "        0.01282864, 0.01116667, 0.01105704, 0.01165595, 0.01115441,\n",
       "        0.01137242, 0.01189718, 0.01230979, 0.01162434, 0.0105207 ,\n",
       "        0.01083808, 0.01244783, 0.01530747, 0.01210775, 0.01099038,\n",
       "        0.01114831, 0.0117763 , 0.00997996, 0.01128888, 0.01047168,\n",
       "        0.01237259, 0.01306081, 0.01139374, 0.01209097, 0.01091523,\n",
       "        0.01149859, 0.01745586, 0.012429  , 0.0130023 , 0.01151619,\n",
       "        0.01146779, 0.01172037, 0.01193976, 0.01331472, 0.01125193,\n",
       "        0.01221366, 0.01352277, 0.01049986, 0.01165967, 0.01047373,\n",
       "        0.01329055, 0.01318374, 0.01165595, 0.01367822, 0.0147954 ,\n",
       "        0.00992703, 0.01024694, 0.01380386, 0.01167736, 0.01306686,\n",
       "        0.01293373, 0.01192446, 0.0113647 , 0.01271267, 0.01298461,\n",
       "        0.01025987, 0.01013637, 0.0098577 , 0.01018481, 0.01207013,\n",
       "        0.01046128, 0.00902295, 0.01522865, 0.01045008, 0.01208239,\n",
       "        0.0120029 , 0.01022658, 0.01433711, 0.01244631, 0.01303258,\n",
       "        0.01144133, 0.01217828, 0.0107677 , 0.01356015, 0.01358871,\n",
       "        0.01067719, 0.00948958, 0.00902772, 0.01035538, 0.00943069,\n",
       "        0.01073871, 0.00982118, 0.00949674, 0.00969014, 0.01082964,\n",
       "        0.01138806, 0.01033158, 0.01056256, 0.01323504, 0.01110635,\n",
       "        0.012713  , 0.01118851, 0.01480083, 0.01223149, 0.01446166,\n",
       "        0.01013699, 0.00881977, 0.01135731, 0.01178741, 0.00896688,\n",
       "        0.01194181, 0.0103066 , 0.00967045, 0.01096416, 0.01155086,\n",
       "        0.01111808, 0.009624  , 0.0127645 , 0.01142945, 0.01091681,\n",
       "        0.01198497, 0.00979309, 0.01198006, 0.01040497, 0.01125093,\n",
       "        0.01244369, 0.01272383, 0.01047907, 0.00989542, 0.00928373,\n",
       "        0.01008911, 0.01101928, 0.01096683, 0.00990481, 0.0110424 ,\n",
       "        0.01232357, 0.0101707 , 0.01138844, 0.00977798, 0.01202545,\n",
       "        0.01194539, 0.01037931, 0.01248708, 0.01230202, 0.01116099,\n",
       "        0.01511188, 0.00954256, 0.00928493, 0.01041951, 0.00993948,\n",
       "        0.01440034, 0.0139297 , 0.0095212 , 0.01076736, 0.01170087,\n",
       "        0.01060462, 0.01351504, 0.01075482, 0.0108542 , 0.01427951,\n",
       "        0.01132092, 0.01009889, 0.0111372 , 0.01381369, 0.01416583,\n",
       "        0.01010909, 0.01043763, 0.00980721, 0.01195803, 0.01030426,\n",
       "        0.01149621, 0.00901461, 0.00989237, 0.01043639, 0.01052542,\n",
       "        0.01161423, 0.00960646, 0.01048555, 0.01153703, 0.0119606 ,\n",
       "        0.01305928, 0.0121542 , 0.01001563, 0.01734042, 0.01219449,\n",
       "        0.01038246, 0.01173964, 0.00946422, 0.01126976, 0.01002321,\n",
       "        0.01207733, 0.01109262, 0.00992303, 0.01049376, 0.01072097,\n",
       "        0.01051016, 0.01099796, 0.01074028, 0.0140512 , 0.01381302,\n",
       "        0.0117559 , 0.01033196, 0.01056633, 0.01213207, 0.01171331,\n",
       "        0.00995941, 0.01003337, 0.00956645, 0.01072249, 0.01048808,\n",
       "        0.01011   , 0.01100922, 0.01229987, 0.01286654, 0.0107038 ,\n",
       "        0.0106729 , 0.01035752, 0.01262951, 0.01166911, 0.01160445,\n",
       "        0.01292911, 0.01168652, 0.01239591, 0.0129005 , 0.01191726,\n",
       "        0.01460786, 0.00966463, 0.00995884, 0.01375518, 0.00965495,\n",
       "        0.00948982, 0.01166077, 0.01098318, 0.01077685, 0.01179047,\n",
       "        0.0113658 , 0.01020589, 0.01105132, 0.01114163, 0.01115551,\n",
       "        0.01236267, 0.01049519, 0.01167393, 0.01124005, 0.01235261,\n",
       "        0.01235085, 0.00956364, 0.00860868, 0.01057448, 0.00925455,\n",
       "        0.01202979, 0.00956826, 0.010045  , 0.0119348 , 0.01645842,\n",
       "        0.01116648, 0.01016717, 0.01212177, 0.01118321, 0.01122637,\n",
       "        0.0153872 , 0.01018834, 0.01101775, 0.012468  , 0.01121883,\n",
       "        0.01140261, 0.01111212, 0.01002908, 0.01042562, 0.01050549,\n",
       "        0.01378517, 0.01281838, 0.01086435, 0.00954137, 0.01081319,\n",
       "        0.01192269, 0.01026096, 0.01088147, 0.01253839, 0.01181684,\n",
       "        0.01531596, 0.01061296, 0.0114706 , 0.01250834, 0.01213684,\n",
       "        0.00960822, 0.01138282, 0.00941224, 0.01124144, 0.01248274,\n",
       "        0.01325321, 0.01066794, 0.01175408, 0.01478767, 0.01088824,\n",
       "        0.01216569, 0.01287084, 0.00993962, 0.011693  , 0.01222157,\n",
       "        0.01228147, 0.01256127, 0.01061015, 0.01269622, 0.01203361,\n",
       "        0.01241655, 0.01405697, 0.01089053, 0.01097174, 0.00970292,\n",
       "        0.01014719, 0.01046972, 0.01020751, 0.01103115, 0.01069756,\n",
       "        0.0124084 , 0.01142917, 0.01045666, 0.01175423, 0.0140533 ,\n",
       "        0.01234961, 0.01095762, 0.01233487, 0.01927457, 0.01317911,\n",
       "        0.0119698 , 0.01011481, 0.01088114, 0.01055942, 0.01048031,\n",
       "        0.00937028, 0.00929818, 0.01101317, 0.01200967, 0.01030145,\n",
       "        0.0113153 , 0.01125703, 0.01096931, 0.01183887, 0.01128445,\n",
       "        0.01139283, 0.01293435, 0.01551905, 0.01073232, 0.01346703,\n",
       "        0.01041608, 0.00853949, 0.01030765, 0.01042767, 0.00930767,\n",
       "        0.01348844, 0.00995884, 0.01083736, 0.01098986, 0.01299648,\n",
       "        0.01132226, 0.01045775, 0.01125917, 0.01269197, 0.01044078,\n",
       "        0.01247745, 0.0113184 , 0.01301255, 0.01258397, 0.01183028,\n",
       "        0.01364956, 0.00988336, 0.00938015, 0.00984521, 0.01410599,\n",
       "        0.01106601, 0.01292706, 0.01028566, 0.0124846 , 0.01185708,\n",
       "        0.01052337, 0.01153316, 0.01212726, 0.0134007 , 0.01155539,\n",
       "        0.01378226, 0.01110587, 0.01241727, 0.01284165, 0.01172247]),\n",
       " 'std_score_time': array([0.00152945, 0.00696918, 0.00106941, 0.00080346, 0.00058251,\n",
       "        0.00092897, 0.00391485, 0.00023883, 0.00114987, 0.00617418,\n",
       "        0.0053891 , 0.00189203, 0.01230146, 0.00592625, 0.00402636,\n",
       "        0.00985249, 0.00485305, 0.0189076 , 0.00932666, 0.00383031,\n",
       "        0.00224984, 0.00100176, 0.01527009, 0.00314402, 0.00963484,\n",
       "        0.00184253, 0.01464988, 0.00953684, 0.00338194, 0.0014152 ,\n",
       "        0.00308381, 0.00308893, 0.02546513, 0.00534094, 0.00758624,\n",
       "        0.00385775, 0.00172236, 0.01413656, 0.00514838, 0.00749144,\n",
       "        0.11260618, 0.00309937, 0.02257069, 0.00195818, 0.0012594 ,\n",
       "        0.00184953, 0.0045496 , 0.00418636, 0.00920652, 0.00336572,\n",
       "        0.00556897, 0.00197655, 0.00504783, 0.00537973, 0.00180739,\n",
       "        0.00114678, 0.06024337, 0.00091155, 0.00169714, 0.02580095,\n",
       "        0.05756568, 0.0034105 , 0.00412668, 0.00254937, 0.00186363,\n",
       "        0.00234855, 0.00205144, 0.00391962, 0.00205456, 0.00191843,\n",
       "        0.00192556, 0.00189271, 0.0018168 , 0.02490288, 0.00207454,\n",
       "        0.0112468 , 0.0007567 , 0.00272236, 0.00235043, 0.00138835,\n",
       "        0.00502445, 0.00305347, 0.00330985, 0.00233075, 0.00092359,\n",
       "        0.00100112, 0.00075608, 0.00411573, 0.00072467, 0.00161206,\n",
       "        0.00441359, 0.0011952 , 0.00298012, 0.00163055, 0.00038168,\n",
       "        0.00110682, 0.00267531, 0.0005564 , 0.00182521, 0.00171019,\n",
       "        0.00295954, 0.00161077, 0.00152609, 0.00511346, 0.00294731,\n",
       "        0.00088073, 0.00043221, 0.00037392, 0.00170951, 0.00121493,\n",
       "        0.00084294, 0.00073458, 0.00132163, 0.0013081 , 0.00136951,\n",
       "        0.00229353, 0.00081074, 0.00021005, 0.00055681, 0.00094527,\n",
       "        0.0037352 , 0.00165332, 0.00131571, 0.00265603, 0.00237446,\n",
       "        0.00107215, 0.00038894, 0.00191623, 0.00273773, 0.00060689,\n",
       "        0.0007345 , 0.00073303, 0.00182145, 0.0025228 , 0.00163791,\n",
       "        0.00145283, 0.00027728, 0.00091344, 0.00074752, 0.00243469,\n",
       "        0.00410114, 0.00126929, 0.00240206, 0.00196543, 0.00446518,\n",
       "        0.00158021, 0.00342872, 0.01439245, 0.00251617, 0.00367693,\n",
       "        0.00117526, 0.00106887, 0.00210533, 0.00049469, 0.00136619,\n",
       "        0.00223396, 0.00131399, 0.0011165 , 0.00402656, 0.00082042,\n",
       "        0.00123115, 0.00583957, 0.00103504, 0.00071706, 0.00229509,\n",
       "        0.00173145, 0.00204079, 0.00125138, 0.00173806, 0.00246902,\n",
       "        0.00365721, 0.00179413, 0.00095454, 0.00459665, 0.00414467,\n",
       "        0.00177309, 0.00187639, 0.00247847, 0.00419456, 0.00071396,\n",
       "        0.00084208, 0.00187735, 0.00190722, 0.00123026, 0.00074465,\n",
       "        0.00099675, 0.00109612, 0.00183601, 0.0018955 , 0.00293323,\n",
       "        0.00105459, 0.00142543, 0.00187606, 0.00129304, 0.00096408,\n",
       "        0.00183451, 0.00133938, 0.00117527, 0.00078583, 0.00097   ,\n",
       "        0.00636011, 0.00081814, 0.00088193, 0.00130749, 0.00106809,\n",
       "        0.00145921, 0.00568037, 0.00520861, 0.00059815, 0.00119351,\n",
       "        0.0025126 , 0.00021019, 0.00024443, 0.00175081, 0.00116092,\n",
       "        0.00171096, 0.00063977, 0.00267612, 0.00417283, 0.00137914,\n",
       "        0.0007824 , 0.00145317, 0.00038093, 0.00086691, 0.00321874,\n",
       "        0.00255522, 0.00224613, 0.00131954, 0.00170556, 0.00136207,\n",
       "        0.00125636, 0.00095788, 0.00179493, 0.00119078, 0.00141505,\n",
       "        0.00158029, 0.00213365, 0.00379298, 0.00208489, 0.00124729,\n",
       "        0.00195949, 0.00188969, 0.00133411, 0.00396919, 0.00137981,\n",
       "        0.0037461 , 0.00400564, 0.00161321, 0.00187625, 0.00138293,\n",
       "        0.0010618 , 0.00556062, 0.00442678, 0.00269538, 0.00183434,\n",
       "        0.00105857, 0.00145912, 0.00102827, 0.00203107, 0.00065017,\n",
       "        0.00394942, 0.00521966, 0.00111858, 0.00197893, 0.0009203 ,\n",
       "        0.00303816, 0.00247001, 0.00132645, 0.00318046, 0.00332763,\n",
       "        0.00039158, 0.0011852 , 0.00460722, 0.0008512 , 0.00204193,\n",
       "        0.0026157 , 0.00183542, 0.0009787 , 0.00082033, 0.00235929,\n",
       "        0.00159189, 0.00174096, 0.00122002, 0.00197172, 0.001803  ,\n",
       "        0.00112917, 0.00043755, 0.00450072, 0.00094204, 0.00162203,\n",
       "        0.00159869, 0.00076607, 0.00419735, 0.0021563 , 0.00212189,\n",
       "        0.00114492, 0.00163753, 0.00081691, 0.00190966, 0.00206444,\n",
       "        0.00109949, 0.00082578, 0.00076117, 0.00291338, 0.00098321,\n",
       "        0.00152182, 0.00150694, 0.00091378, 0.00085302, 0.00069419,\n",
       "        0.0015615 , 0.00104495, 0.00036385, 0.00422319, 0.00061921,\n",
       "        0.00365602, 0.00203751, 0.00476465, 0.00203552, 0.00402262,\n",
       "        0.00124432, 0.00063754, 0.00241173, 0.00310711, 0.00066825,\n",
       "        0.00404523, 0.00170402, 0.00078746, 0.00049908, 0.00245585,\n",
       "        0.00143979, 0.00114419, 0.00170539, 0.00069141, 0.00097494,\n",
       "        0.00187507, 0.0007862 , 0.00122622, 0.00082806, 0.00131459,\n",
       "        0.00263567, 0.00276973, 0.00185597, 0.00105666, 0.00092921,\n",
       "        0.00077582, 0.0007399 , 0.00074818, 0.00120475, 0.00172026,\n",
       "        0.00398684, 0.00079729, 0.00149405, 0.00101432, 0.00090298,\n",
       "        0.00222225, 0.00112475, 0.00104385, 0.00068473, 0.00108782,\n",
       "        0.00765604, 0.00046964, 0.00075283, 0.00053752, 0.00086395,\n",
       "        0.00528353, 0.00346126, 0.00023311, 0.00075762, 0.00258727,\n",
       "        0.00122295, 0.00566613, 0.00188624, 0.00083379, 0.00399272,\n",
       "        0.00110699, 0.00081296, 0.00113632, 0.00421351, 0.00330319,\n",
       "        0.00112811, 0.00179394, 0.00072361, 0.00149088, 0.00040264,\n",
       "        0.00313991, 0.00041665, 0.00080761, 0.00104678, 0.00087219,\n",
       "        0.00190423, 0.00061211, 0.00152748, 0.00085701, 0.00302443,\n",
       "        0.00197444, 0.00328083, 0.0004495 , 0.01017429, 0.00054567,\n",
       "        0.00084909, 0.00265207, 0.00185964, 0.00196327, 0.00055921,\n",
       "        0.00134432, 0.00195087, 0.00069668, 0.00086204, 0.001159  ,\n",
       "        0.00077279, 0.00245686, 0.00118875, 0.00396198, 0.00321599,\n",
       "        0.00083612, 0.00082231, 0.00091442, 0.00117065, 0.00102484,\n",
       "        0.00089142, 0.00080047, 0.00099803, 0.0016302 , 0.00081224,\n",
       "        0.00065129, 0.00130971, 0.00381379, 0.00231527, 0.00106446,\n",
       "        0.00116305, 0.00091255, 0.003207  , 0.00157974, 0.00182832,\n",
       "        0.00107569, 0.00116382, 0.0034218 , 0.00208217, 0.00170119,\n",
       "        0.00617679, 0.00094641, 0.00087347, 0.0065604 , 0.00080158,\n",
       "        0.00059526, 0.00322878, 0.00084987, 0.00157686, 0.00171888,\n",
       "        0.0011699 , 0.00095492, 0.00094747, 0.00114553, 0.00078232,\n",
       "        0.00155553, 0.00095006, 0.00144846, 0.00144128, 0.00260942,\n",
       "        0.0028141 , 0.00118026, 0.00083741, 0.00105671, 0.00095894,\n",
       "        0.00205051, 0.00090213, 0.00061213, 0.00205673, 0.00657795,\n",
       "        0.00068932, 0.00083238, 0.00357123, 0.00127681, 0.00114138,\n",
       "        0.00375983, 0.00136491, 0.00120575, 0.0008552 , 0.00113819,\n",
       "        0.00352437, 0.00274406, 0.00102689, 0.00019548, 0.00108778,\n",
       "        0.00202373, 0.00489433, 0.00184563, 0.0006354 , 0.00124567,\n",
       "        0.00167499, 0.00093564, 0.00084874, 0.00235791, 0.00087027,\n",
       "        0.00596293, 0.00067895, 0.00171435, 0.00114228, 0.00083771,\n",
       "        0.00100827, 0.00444637, 0.00071873, 0.00179486, 0.0027472 ,\n",
       "        0.00262122, 0.00169549, 0.00215473, 0.00789109, 0.00043244,\n",
       "        0.00175955, 0.00550987, 0.0005966 , 0.00107078, 0.00086768,\n",
       "        0.0021877 , 0.00366064, 0.00036511, 0.00073971, 0.00113568,\n",
       "        0.00241821, 0.00321917, 0.00133396, 0.0014066 , 0.00079696,\n",
       "        0.00028328, 0.00068901, 0.00081183, 0.00137094, 0.00130026,\n",
       "        0.00077436, 0.00182725, 0.0010224 , 0.0007499 , 0.00448238,\n",
       "        0.00075905, 0.00090369, 0.00108565, 0.01567798, 0.00179041,\n",
       "        0.00445442, 0.00182881, 0.00144529, 0.00055668, 0.00056348,\n",
       "        0.00075487, 0.00065687, 0.00074868, 0.00166175, 0.00096914,\n",
       "        0.00113786, 0.00161252, 0.00084828, 0.00125819, 0.00061198,\n",
       "        0.00086504, 0.00143348, 0.00418165, 0.00092323, 0.00492914,\n",
       "        0.00057029, 0.00044678, 0.00061225, 0.00057218, 0.00058211,\n",
       "        0.00541343, 0.00100547, 0.00127983, 0.00168487, 0.00211418,\n",
       "        0.00079546, 0.00107916, 0.00083444, 0.00090601, 0.00110902,\n",
       "        0.00097413, 0.00150947, 0.00225007, 0.00422304, 0.00038391,\n",
       "        0.00522773, 0.0006882 , 0.00048945, 0.00060716, 0.00445088,\n",
       "        0.00116409, 0.00366725, 0.00049283, 0.00229645, 0.00067106,\n",
       "        0.00129871, 0.00057733, 0.00311294, 0.00418286, 0.00152101,\n",
       "        0.00315305, 0.00073173, 0.00306011, 0.00111507, 0.00155048]),\n",
       " 'param_learning_rate': masked_array(data=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value=1e+20),\n",
       " 'param_max_depth': masked_array(data=[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "                    -1, -1, -1, -1, -1, -1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, -1, -1,\n",
       "                    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "                    -1, -1, -1, -1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, -1, -1, -1, -1,\n",
       "                    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "                    -1, -1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1,\n",
       "                    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "                    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "                    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value=999999),\n",
       " 'param_n_estimators': masked_array(data=[50, 50, 50, 50, 50, 100, 100, 100, 100, 100, 150, 150,\n",
       "                    150, 150, 150, 200, 200, 200, 200, 200, 50, 50, 50, 50,\n",
       "                    50, 100, 100, 100, 100, 100, 150, 150, 150, 150, 150,\n",
       "                    200, 200, 200, 200, 200, 50, 50, 50, 50, 50, 100, 100,\n",
       "                    100, 100, 100, 150, 150, 150, 150, 150, 200, 200, 200,\n",
       "                    200, 200, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100,\n",
       "                    150, 150, 150, 150, 150, 200, 200, 200, 200, 200, 50,\n",
       "                    50, 50, 50, 50, 100, 100, 100, 100, 100, 150, 150, 150,\n",
       "                    150, 150, 200, 200, 200, 200, 200, 50, 50, 50, 50, 50,\n",
       "                    100, 100, 100, 100, 100, 150, 150, 150, 150, 150, 200,\n",
       "                    200, 200, 200, 200, 50, 50, 50, 50, 50, 100, 100, 100,\n",
       "                    100, 100, 150, 150, 150, 150, 150, 200, 200, 200, 200,\n",
       "                    200, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100, 150,\n",
       "                    150, 150, 150, 150, 200, 200, 200, 200, 200, 50, 50,\n",
       "                    50, 50, 50, 100, 100, 100, 100, 100, 150, 150, 150,\n",
       "                    150, 150, 200, 200, 200, 200, 200, 50, 50, 50, 50, 50,\n",
       "                    100, 100, 100, 100, 100, 150, 150, 150, 150, 150, 200,\n",
       "                    200, 200, 200, 200, 50, 50, 50, 50, 50, 100, 100, 100,\n",
       "                    100, 100, 150, 150, 150, 150, 150, 200, 200, 200, 200,\n",
       "                    200, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100, 150,\n",
       "                    150, 150, 150, 150, 200, 200, 200, 200, 200, 50, 50,\n",
       "                    50, 50, 50, 100, 100, 100, 100, 100, 150, 150, 150,\n",
       "                    150, 150, 200, 200, 200, 200, 200, 50, 50, 50, 50, 50,\n",
       "                    100, 100, 100, 100, 100, 150, 150, 150, 150, 150, 200,\n",
       "                    200, 200, 200, 200, 50, 50, 50, 50, 50, 100, 100, 100,\n",
       "                    100, 100, 150, 150, 150, 150, 150, 200, 200, 200, 200,\n",
       "                    200, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100, 150,\n",
       "                    150, 150, 150, 150, 200, 200, 200, 200, 200, 50, 50,\n",
       "                    50, 50, 50, 100, 100, 100, 100, 100, 150, 150, 150,\n",
       "                    150, 150, 200, 200, 200, 200, 200, 50, 50, 50, 50, 50,\n",
       "                    100, 100, 100, 100, 100, 150, 150, 150, 150, 150, 200,\n",
       "                    200, 200, 200, 200, 50, 50, 50, 50, 50, 100, 100, 100,\n",
       "                    100, 100, 150, 150, 150, 150, 150, 200, 200, 200, 200,\n",
       "                    200, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100, 150,\n",
       "                    150, 150, 150, 150, 200, 200, 200, 200, 200, 50, 50,\n",
       "                    50, 50, 50, 100, 100, 100, 100, 100, 150, 150, 150,\n",
       "                    150, 150, 200, 200, 200, 200, 200, 50, 50, 50, 50, 50,\n",
       "                    100, 100, 100, 100, 100, 150, 150, 150, 150, 150, 200,\n",
       "                    200, 200, 200, 200, 50, 50, 50, 50, 50, 100, 100, 100,\n",
       "                    100, 100, 150, 150, 150, 150, 150, 200, 200, 200, 200,\n",
       "                    200, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100, 150,\n",
       "                    150, 150, 150, 150, 200, 200, 200, 200, 200, 50, 50,\n",
       "                    50, 50, 50, 100, 100, 100, 100, 100, 150, 150, 150,\n",
       "                    150, 150, 200, 200, 200, 200, 200, 50, 50, 50, 50, 50,\n",
       "                    100, 100, 100, 100, 100, 150, 150, 150, 150, 150, 200,\n",
       "                    200, 200, 200, 200, 50, 50, 50, 50, 50, 100, 100, 100,\n",
       "                    100, 100, 150, 150, 150, 150, 150, 200, 200, 200, 200,\n",
       "                    200, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100, 150,\n",
       "                    150, 150, 150, 150, 200, 200, 200, 200, 200, 50, 50,\n",
       "                    50, 50, 50, 100, 100, 100, 100, 100, 150, 150, 150,\n",
       "                    150, 150, 200, 200, 200, 200, 200, 50, 50, 50, 50, 50,\n",
       "                    100, 100, 100, 100, 100, 150, 150, 150, 150, 150, 200,\n",
       "                    200, 200, 200, 200],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value=999999),\n",
       " 'param_num_leaves': masked_array(data=[31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32,\n",
       "                    64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16,\n",
       "                    32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8,\n",
       "                    16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31,\n",
       "                    8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64,\n",
       "                    31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32,\n",
       "                    64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16,\n",
       "                    32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8,\n",
       "                    16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31,\n",
       "                    8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64,\n",
       "                    31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32,\n",
       "                    64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16,\n",
       "                    32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8,\n",
       "                    16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31,\n",
       "                    8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64,\n",
       "                    31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32,\n",
       "                    64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16,\n",
       "                    32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8,\n",
       "                    16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31,\n",
       "                    8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64,\n",
       "                    31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32,\n",
       "                    64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16,\n",
       "                    32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8,\n",
       "                    16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31,\n",
       "                    8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64,\n",
       "                    31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32,\n",
       "                    64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16,\n",
       "                    32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8,\n",
       "                    16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31,\n",
       "                    8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64,\n",
       "                    31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32,\n",
       "                    64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16,\n",
       "                    32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8,\n",
       "                    16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31,\n",
       "                    8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64,\n",
       "                    31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32,\n",
       "                    64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16,\n",
       "                    32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8,\n",
       "                    16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31,\n",
       "                    8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64,\n",
       "                    31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32,\n",
       "                    64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64, 31, 8, 16,\n",
       "                    32, 64, 31, 8, 16, 32, 64, 31, 8, 16, 32, 64],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value=999999),\n",
       " 'params': [{'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': -1, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 1.0, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 1.0, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 5, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 5, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 5, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 1.0, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 1.0, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 6, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 6, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 1.0, 'max_depth': 6, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 1.0,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': -1, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 0.5, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 0.5, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 6, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 6, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.5, 'max_depth': 6, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 16},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 32},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': -1,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 50, 'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 50,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 100,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 150,\n",
       "   'num_leaves': 64},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 31},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 8},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 16},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 32},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'n_estimators': 200,\n",
       "   'num_leaves': 64}],\n",
       " 'split0_test_score': array([0.77690485, 0.83127275, 0.79157302, 0.77285912, 0.76438484,\n",
       "        0.76197301, 0.8126288 , 0.77105095, 0.75395463, 0.74577329,\n",
       "        0.75546591, 0.80776831, 0.75105061, 0.74892921, 0.7337775 ,\n",
       "        0.74934697, 0.79887191, 0.74371851, 0.7443116 , 0.73120724,\n",
       "        0.84534128, 0.84534128, 0.84534128, 0.84534128, 0.84534128,\n",
       "        0.83008322, 0.83008322, 0.83008322, 0.83008322, 0.83008322,\n",
       "        0.81623198, 0.81623198, 0.81623198, 0.81623198, 0.81623198,\n",
       "        0.80735274, 0.80735274, 0.80735274, 0.80735274, 0.80735274,\n",
       "        0.83027046, 0.84200175, 0.83027046, 0.83027046, 0.83027046,\n",
       "        0.80920883, 0.82432005, 0.80920883, 0.80920883, 0.80920883,\n",
       "        0.79044499, 0.81517563, 0.79044499, 0.79044499, 0.79044499,\n",
       "        0.78000613, 0.80604623, 0.78000613, 0.78000613, 0.78000613,\n",
       "        0.80924303, 0.83091939, 0.80355499, 0.80924303, 0.80924303,\n",
       "        0.79490288, 0.80675699, 0.77906606, 0.79490288, 0.79490288,\n",
       "        0.77728654, 0.78622466, 0.76994827, 0.77728654, 0.77728654,\n",
       "        0.77093043, 0.77292027, 0.76423373, 0.77093043, 0.77093043,\n",
       "        0.77724651, 0.83251397, 0.80290366, 0.77724651, 0.77724651,\n",
       "        0.7496551 , 0.81297303, 0.7778267 , 0.7496551 , 0.7496551 ,\n",
       "        0.74252293, 0.80133782, 0.76805099, 0.74252293, 0.74252293,\n",
       "        0.73735645, 0.78977837, 0.75934024, 0.73735645, 0.73735645,\n",
       "        0.81653836, 0.85122998, 0.83482075, 0.81835876, 0.82484016,\n",
       "        0.79653279, 0.83775086, 0.80837312, 0.79090786, 0.8018772 ,\n",
       "        0.78816388, 0.82768154, 0.79710245, 0.78001225, 0.79043461,\n",
       "        0.77874095, 0.81418858, 0.78665549, 0.77211635, 0.78133036,\n",
       "        0.86078954, 0.86078954, 0.86078954, 0.86078954, 0.86078954,\n",
       "        0.8492297 , 0.8492297 , 0.8492297 , 0.8492297 , 0.8492297 ,\n",
       "        0.84209506, 0.84209506, 0.84209506, 0.84209506, 0.84209506,\n",
       "        0.83582453, 0.83582453, 0.83582453, 0.83582453, 0.83582453,\n",
       "        0.85509724, 0.86212287, 0.85509724, 0.85509724, 0.85509724,\n",
       "        0.84013559, 0.84633726, 0.84013559, 0.84013559, 0.84013559,\n",
       "        0.82700134, 0.83962288, 0.82700134, 0.82700134, 0.82700134,\n",
       "        0.8191626 , 0.82746375, 0.8191626 , 0.8191626 , 0.8191626 ,\n",
       "        0.84690075, 0.85797421, 0.84604875, 0.84690075, 0.84690075,\n",
       "        0.82578779, 0.84356422, 0.8263134 , 0.82578779, 0.82578779,\n",
       "        0.81086481, 0.83117814, 0.81108174, 0.81086481, 0.81086481,\n",
       "        0.80188657, 0.82008385, 0.80116471, 0.80188657, 0.80188657,\n",
       "        0.84763647, 0.85655246, 0.83952521, 0.84763647, 0.84763647,\n",
       "        0.82671155, 0.84401746, 0.82464899, 0.82671155, 0.82671155,\n",
       "        0.80945935, 0.83450818, 0.81301324, 0.80945935, 0.80945935,\n",
       "        0.80157136, 0.82492284, 0.80028392, 0.80157136, 0.80157136,\n",
       "        0.86701368, 0.87855093, 0.87464153, 0.86464848, 0.86828278,\n",
       "        0.85327645, 0.87310384, 0.8629927 , 0.85168828, 0.85636492,\n",
       "        0.84200223, 0.86990987, 0.85745333, 0.842244  , 0.84571609,\n",
       "        0.83396126, 0.86704341, 0.85163536, 0.83473232, 0.83832337,\n",
       "        0.87989524, 0.87989524, 0.87989524, 0.87989524, 0.87989524,\n",
       "        0.87558418, 0.87558418, 0.87558418, 0.87558418, 0.87558418,\n",
       "        0.87340067, 0.87340067, 0.87340067, 0.87340067, 0.87340067,\n",
       "        0.87064111, 0.87064111, 0.87064111, 0.87064111, 0.87064111,\n",
       "        0.87817704, 0.87922871, 0.87817704, 0.87817704, 0.87817704,\n",
       "        0.87119985, 0.87475563, 0.87119985, 0.87119985, 0.87119985,\n",
       "        0.86887245, 0.8714407 , 0.86887245, 0.86887245, 0.86887245,\n",
       "        0.86600159, 0.86930342, 0.86600159, 0.86600159, 0.86600159,\n",
       "        0.87552065, 0.87799699, 0.87784823, 0.87552065, 0.87552065,\n",
       "        0.86873706, 0.87377715, 0.87086809, 0.86873706, 0.86873706,\n",
       "        0.86471416, 0.87026412, 0.86851271, 0.86471416, 0.86471416,\n",
       "        0.86110761, 0.86822637, 0.86334123, 0.86110761, 0.86110761,\n",
       "        0.87394639, 0.87728986, 0.87661236, 0.87394639, 0.87394639,\n",
       "        0.8659289 , 0.87249835, 0.86757223, 0.8659289 , 0.8659289 ,\n",
       "        0.86006002, 0.86888232, 0.86309524, 0.86006002, 0.86006002,\n",
       "        0.85544458, 0.86657241, 0.85820638, 0.85544458, 0.85544458,\n",
       "        0.56572112, 0.56586711, 0.56578746, 0.56568242, 0.56571174,\n",
       "        0.7668418 , 0.76831972, 0.76818913, 0.76658832, 0.7665878 ,\n",
       "        0.84006726, 0.84146971, 0.84168919, 0.83978675, 0.83940678,\n",
       "        0.8650167 , 0.86764959, 0.8672888 , 0.86479675, 0.86449826,\n",
       "        0.56585852, 0.56585852, 0.56585852, 0.56585852, 0.56585852,\n",
       "        0.76874611, 0.76874611, 0.76874611, 0.76874611, 0.76874611,\n",
       "        0.84200665, 0.84200665, 0.84200665, 0.84200665, 0.84200665,\n",
       "        0.86716447, 0.86716447, 0.86716447, 0.86716447, 0.86716447,\n",
       "        0.56568745, 0.56586711, 0.56568745, 0.56568745, 0.56568745,\n",
       "        0.76788279, 0.76831972, 0.76788279, 0.76788279, 0.76788279,\n",
       "        0.84075025, 0.84146971, 0.84075025, 0.84075025, 0.84075025,\n",
       "        0.8663111 , 0.86764959, 0.8663111 , 0.8663111 , 0.8663111 ,\n",
       "        0.56559267, 0.56586711, 0.56566404, 0.56559267, 0.56559267,\n",
       "        0.76743162, 0.76831972, 0.76763469, 0.76743162, 0.76743162,\n",
       "        0.84056852, 0.84146971, 0.84068828, 0.84056852, 0.84056852,\n",
       "        0.86603556, 0.86764959, 0.86650036, 0.86603556, 0.86603556,\n",
       "        0.56507908, 0.56586711, 0.56569699, 0.56507902, 0.56507902,\n",
       "        0.76664261, 0.76831972, 0.76799632, 0.76664358, 0.76664358,\n",
       "        0.83980624, 0.84146971, 0.84169596, 0.83980732, 0.83980732,\n",
       "        0.86468264, 0.86764959, 0.86729648, 0.86468375, 0.86468375,\n",
       "        0.84075086, 0.84268902, 0.84192276, 0.8406336 , 0.84031604,\n",
       "        0.87496142, 0.88009265, 0.87956358, 0.87555339, 0.87419614,\n",
       "        0.86999727, 0.87970389, 0.87719046, 0.86948196, 0.87024515,\n",
       "        0.86545119, 0.87633962, 0.87330161, 0.86479133, 0.8641179 ,\n",
       "        0.84318863, 0.84318863, 0.84318863, 0.84318863, 0.84318863,\n",
       "        0.88026302, 0.88026302, 0.88026302, 0.88026302, 0.88026302,\n",
       "        0.88126798, 0.88126798, 0.88126798, 0.88126798, 0.88126798,\n",
       "        0.87925913, 0.87925913, 0.87925913, 0.87925913, 0.87925913,\n",
       "        0.84176433, 0.84268902, 0.84176433, 0.84176433, 0.84176433,\n",
       "        0.87863672, 0.87989638, 0.87863672, 0.87863672, 0.87863672,\n",
       "        0.8784017 , 0.88053807, 0.8784017 , 0.8784017 , 0.8784017 ,\n",
       "        0.87715261, 0.87798229, 0.87715261, 0.87715261, 0.87715261,\n",
       "        0.84141599, 0.84268902, 0.84160862, 0.84141599, 0.84141599,\n",
       "        0.87764301, 0.88009265, 0.87872632, 0.87764301, 0.87764301,\n",
       "        0.87683698, 0.8799078 , 0.87799602, 0.87683698, 0.87683698,\n",
       "        0.87478391, 0.87811574, 0.87546997, 0.87478391, 0.87478391,\n",
       "        0.84089997, 0.84268902, 0.84277732, 0.84089997, 0.84089997,\n",
       "        0.87647373, 0.88009265, 0.87947008, 0.87647373, 0.87647373,\n",
       "        0.87504285, 0.87997793, 0.87758069, 0.87504285, 0.87504285,\n",
       "        0.87217196, 0.87679856, 0.87480929, 0.87217196, 0.87217196,\n",
       "        0.87318433, 0.87692701, 0.87638172, 0.87329423, 0.87333318,\n",
       "        0.86805103, 0.87882183, 0.87391847, 0.8681467 , 0.86819312,\n",
       "        0.86064741, 0.87534835, 0.86758516, 0.86010109, 0.86049979,\n",
       "        0.85425194, 0.87381657, 0.86385337, 0.85377453, 0.85386429,\n",
       "        0.87706238, 0.87706238, 0.87706238, 0.87706238, 0.87706238,\n",
       "        0.88033044, 0.88033044, 0.88033044, 0.88033044, 0.88033044,\n",
       "        0.87785114, 0.87785114, 0.87785114, 0.87785114, 0.87785114,\n",
       "        0.87606434, 0.87606434, 0.87606434, 0.87606434, 0.87606434,\n",
       "        0.87595282, 0.87692701, 0.87595282, 0.87595282, 0.87595282,\n",
       "        0.87790383, 0.87916465, 0.87790383, 0.87790383, 0.87790383,\n",
       "        0.87550404, 0.87573639, 0.87550404, 0.87550404, 0.87550404,\n",
       "        0.87378711, 0.87359324, 0.87378711, 0.87378711, 0.87378711,\n",
       "        0.87493126, 0.87692701, 0.87562689, 0.87493126, 0.87493126,\n",
       "        0.87714193, 0.87936563, 0.87687877, 0.87714193, 0.87714193,\n",
       "        0.87289792, 0.87629338, 0.87277251, 0.87289792, 0.87289792,\n",
       "        0.86942268, 0.87467023, 0.86980468, 0.86942268, 0.86942268,\n",
       "        0.87469161, 0.87692701, 0.87637242, 0.87469161, 0.87469161,\n",
       "        0.87455988, 0.87873055, 0.87717984, 0.87455988, 0.87455988,\n",
       "        0.87146319, 0.87628602, 0.87311647, 0.87146319, 0.87146319,\n",
       "        0.86796884, 0.87463681, 0.86814104, 0.86796884, 0.86796884]),\n",
       " 'split1_test_score': array([0.72064359, 0.8059425 , 0.73016955, 0.72632592, 0.70336786,\n",
       "        0.68501555, 0.77125085, 0.70909305, 0.70965942, 0.66123483,\n",
       "        0.66525299, 0.75144735, 0.68469054, 0.68715334, 0.65082789,\n",
       "        0.65791449, 0.73432509, 0.67033294, 0.68112636, 0.64335768,\n",
       "        0.80377865, 0.80377865, 0.80377865, 0.80377865, 0.80377865,\n",
       "        0.78583695, 0.78583695, 0.78583695, 0.78583695, 0.78583695,\n",
       "        0.76616308, 0.76616308, 0.76616308, 0.76616308, 0.76616308,\n",
       "        0.75470462, 0.75470462, 0.75470462, 0.75470462, 0.75470462,\n",
       "        0.76699416, 0.7941565 , 0.76699416, 0.76699416, 0.76699416,\n",
       "        0.75383123, 0.76919723, 0.75383123, 0.75383123, 0.75383123,\n",
       "        0.74182507, 0.74732736, 0.74182507, 0.74182507, 0.74182507,\n",
       "        0.73018095, 0.73787418, 0.73018095, 0.73018095, 0.73018095,\n",
       "        0.76531353, 0.76560876, 0.77080087, 0.76531353, 0.76531353,\n",
       "        0.72699192, 0.74760644, 0.73718698, 0.72699192, 0.72699192,\n",
       "        0.69051508, 0.72481148, 0.72496153, 0.69051508, 0.69051508,\n",
       "        0.67632191, 0.71380201, 0.70925824, 0.67632191, 0.67632191,\n",
       "        0.74206   , 0.78700339, 0.71722071, 0.74206   , 0.74206   ,\n",
       "        0.72493681, 0.76294937, 0.67613789, 0.72493681, 0.72493681,\n",
       "        0.70432244, 0.74521381, 0.65487094, 0.70432244, 0.70432244,\n",
       "        0.69225177, 0.73521785, 0.63588131, 0.69225177, 0.69225177,\n",
       "        0.77424284, 0.81835085, 0.79756715, 0.76414843, 0.7791179 ,\n",
       "        0.74673605, 0.79746359, 0.77886811, 0.73929857, 0.74522864,\n",
       "        0.72781858, 0.79156993, 0.75539313, 0.72658608, 0.72691962,\n",
       "        0.71603215, 0.78195602, 0.74008071, 0.71395749, 0.7128282 ,\n",
       "        0.82805443, 0.82805443, 0.82805443, 0.82805443, 0.82805443,\n",
       "        0.81744534, 0.81744534, 0.81744534, 0.81744534, 0.81744534,\n",
       "        0.80459169, 0.80459169, 0.80459169, 0.80459169, 0.80459169,\n",
       "        0.79464714, 0.79464714, 0.79464714, 0.79464714, 0.79464714,\n",
       "        0.81887744, 0.82945759, 0.81887744, 0.81887744, 0.81887744,\n",
       "        0.80413484, 0.8104381 , 0.80413484, 0.80413484, 0.80413484,\n",
       "        0.79067293, 0.79977976, 0.79067293, 0.79067293, 0.79067293,\n",
       "        0.77921072, 0.78785148, 0.77921072, 0.77921072, 0.77921072,\n",
       "        0.81912198, 0.82168305, 0.80315523, 0.81912198, 0.81912198,\n",
       "        0.79791054, 0.80700279, 0.78473134, 0.79791054, 0.79791054,\n",
       "        0.78289682, 0.79168557, 0.76795509, 0.78289682, 0.78289682,\n",
       "        0.77201399, 0.7806265 , 0.75386928, 0.77201399, 0.77201399,\n",
       "        0.80190841, 0.82282688, 0.8057282 , 0.80190841, 0.80190841,\n",
       "        0.77710182, 0.79802609, 0.78199692, 0.77710182, 0.77710182,\n",
       "        0.76157533, 0.78609074, 0.76989752, 0.76157533, 0.76157533,\n",
       "        0.74774944, 0.775561  , 0.75900648, 0.74774944, 0.74774944,\n",
       "        0.83720714, 0.84572177, 0.84382722, 0.83710955, 0.83434854,\n",
       "        0.818533  , 0.84251162, 0.8325633 , 0.81838146, 0.82038394,\n",
       "        0.80894935, 0.83665387, 0.82593196, 0.80688866, 0.80799316,\n",
       "        0.79992254, 0.83244306, 0.8212141 , 0.79739729, 0.79963727,\n",
       "        0.84794006, 0.84794006, 0.84794006, 0.84794006, 0.84794006,\n",
       "        0.84612539, 0.84612539, 0.84612539, 0.84612539, 0.84612539,\n",
       "        0.8424669 , 0.8424669 , 0.8424669 , 0.8424669 , 0.8424669 ,\n",
       "        0.83730075, 0.83730075, 0.83730075, 0.83730075, 0.83730075,\n",
       "        0.84463309, 0.84753338, 0.84463309, 0.84463309, 0.84463309,\n",
       "        0.83977207, 0.84426764, 0.83977207, 0.83977207, 0.83977207,\n",
       "        0.83483484, 0.83974271, 0.83483484, 0.83483484, 0.83483484,\n",
       "        0.83030088, 0.83467018, 0.83030088, 0.83030088, 0.83030088,\n",
       "        0.84121324, 0.84780018, 0.84264706, 0.84121324, 0.84121324,\n",
       "        0.83648655, 0.84449563, 0.83758138, 0.83648655, 0.83648655,\n",
       "        0.83120029, 0.84012566, 0.83253106, 0.83120029, 0.83120029,\n",
       "        0.82415107, 0.83580849, 0.8258247 , 0.82415107, 0.82415107,\n",
       "        0.84383415, 0.84653982, 0.84321378, 0.84383415, 0.84383415,\n",
       "        0.83687164, 0.84082443, 0.83631774, 0.83687164, 0.83687164,\n",
       "        0.82914152, 0.83494098, 0.8293625 , 0.82914152, 0.82914152,\n",
       "        0.82434006, 0.83010091, 0.82149377, 0.82434006, 0.82434006,\n",
       "        0.52297097, 0.50805053, 0.51809053, 0.52297866, 0.52298838,\n",
       "        0.72582015, 0.71315535, 0.72119557, 0.72587582, 0.72599643,\n",
       "        0.80196789, 0.79264049, 0.79826121, 0.80242455, 0.80289234,\n",
       "        0.82966963, 0.82472686, 0.82807622, 0.83030293, 0.83110363,\n",
       "        0.50791659, 0.50791659, 0.50791659, 0.50791659, 0.50791659,\n",
       "        0.71322107, 0.71322107, 0.71322107, 0.71322107, 0.71322107,\n",
       "        0.79385959, 0.79385959, 0.79385959, 0.79385959, 0.79385959,\n",
       "        0.82579456, 0.82579456, 0.82579456, 0.82579456, 0.82579456,\n",
       "        0.51722929, 0.50805053, 0.51722929, 0.51722929, 0.51722929,\n",
       "        0.72087066, 0.71315535, 0.72087066, 0.72087066, 0.72087066,\n",
       "        0.79826559, 0.79264049, 0.79826559, 0.79826559, 0.79826559,\n",
       "        0.8279666 , 0.82472686, 0.8279666 , 0.8279666 , 0.8279666 ,\n",
       "        0.52012914, 0.50805053, 0.5180627 , 0.52012914, 0.52012914,\n",
       "        0.72189319, 0.71315535, 0.72043485, 0.72189319, 0.72189319,\n",
       "        0.79679268, 0.79264049, 0.79697783, 0.79679268, 0.79679268,\n",
       "        0.82595511, 0.82472686, 0.82742478, 0.82595511, 0.82595511,\n",
       "        0.52233966, 0.50805053, 0.51801375, 0.52233966, 0.52233966,\n",
       "        0.72491988, 0.71315535, 0.72063651, 0.72491988, 0.72491988,\n",
       "        0.80140947, 0.79264049, 0.79773268, 0.80140947, 0.80140947,\n",
       "        0.83051329, 0.82472686, 0.82851163, 0.83051329, 0.83051329,\n",
       "        0.80308778, 0.79427979, 0.79955096, 0.80319042, 0.80368462,\n",
       "        0.84001973, 0.84214074, 0.84207925, 0.83926148, 0.84114233,\n",
       "        0.83725445, 0.8476928 , 0.84538918, 0.83778494, 0.83975461,\n",
       "        0.83183045, 0.84659494, 0.84403492, 0.83156125, 0.83279208,\n",
       "        0.79495219, 0.79495219, 0.79495219, 0.79495219, 0.79495219,\n",
       "        0.84265808, 0.84265808, 0.84265808, 0.84265808, 0.84265808,\n",
       "        0.84799024, 0.84799024, 0.84799024, 0.84799024, 0.84799024,\n",
       "        0.84839738, 0.84839738, 0.84839738, 0.84839738, 0.84839738,\n",
       "        0.79976706, 0.79427979, 0.79976706, 0.79976706, 0.79976706,\n",
       "        0.84280307, 0.84245044, 0.84280307, 0.84280307, 0.84280307,\n",
       "        0.8438019 , 0.84687172, 0.8438019 , 0.8438019 , 0.8438019 ,\n",
       "        0.84375482, 0.8463021 , 0.84375482, 0.84375482, 0.84375482,\n",
       "        0.79831491, 0.79427979, 0.79804442, 0.79831491, 0.79831491,\n",
       "        0.83924082, 0.84214074, 0.84199651, 0.83924082, 0.83924082,\n",
       "        0.84073094, 0.84701713, 0.84425499, 0.84073094, 0.84073094,\n",
       "        0.83952094, 0.84633142, 0.84551735, 0.83952094, 0.83952094,\n",
       "        0.80283595, 0.79427979, 0.79908125, 0.80283595, 0.80283595,\n",
       "        0.8437284 , 0.84214074, 0.84292267, 0.8437284 , 0.8437284 ,\n",
       "        0.84515384, 0.84707814, 0.8461503 , 0.84515384, 0.84515384,\n",
       "        0.84438498, 0.84655835, 0.84524414, 0.84438498, 0.84438498,\n",
       "        0.83960847, 0.83755536, 0.83817417, 0.83930587, 0.84041115,\n",
       "        0.8361181 , 0.84774589, 0.8427613 , 0.83559579, 0.83495108,\n",
       "        0.82930839, 0.84674669, 0.83844416, 0.8266471 , 0.82467612,\n",
       "        0.82201565, 0.84399855, 0.83204776, 0.81947125, 0.81600049,\n",
       "        0.8388614 , 0.8388614 , 0.8388614 , 0.8388614 , 0.8388614 ,\n",
       "        0.8479561 , 0.8479561 , 0.8479561 , 0.8479561 , 0.8479561 ,\n",
       "        0.84783003, 0.84783003, 0.84783003, 0.84783003, 0.84783003,\n",
       "        0.84639352, 0.84639352, 0.84639352, 0.84639352, 0.84639352,\n",
       "        0.83975729, 0.83813281, 0.83975729, 0.83975729, 0.83975729,\n",
       "        0.84476939, 0.84695811, 0.84476939, 0.84476939, 0.84476939,\n",
       "        0.84307899, 0.84432269, 0.84307899, 0.84307899, 0.84307899,\n",
       "        0.83883815, 0.84254667, 0.83883815, 0.83883815, 0.83883815,\n",
       "        0.83675009, 0.83755536, 0.83841947, 0.83675009, 0.83675009,\n",
       "        0.84030524, 0.84765098, 0.84335195, 0.84030524, 0.84030524,\n",
       "        0.83928723, 0.84609877, 0.84301538, 0.83928723, 0.83928723,\n",
       "        0.83493042, 0.84388587, 0.83976478, 0.83493042, 0.83493042,\n",
       "        0.83978017, 0.83755536, 0.84002213, 0.83978017, 0.83978017,\n",
       "        0.84413282, 0.84733177, 0.84442759, 0.84413282, 0.84413282,\n",
       "        0.83959627, 0.84732294, 0.84206712, 0.83959627, 0.83959627,\n",
       "        0.83580395, 0.84361841, 0.83808975, 0.83580395, 0.83580395]),\n",
       " 'split2_test_score': array([0.7456392 , 0.76226654, 0.75434247, 0.72580839, 0.72293406,\n",
       "        0.73367824, 0.76958251, 0.73601481, 0.71064827, 0.71522663,\n",
       "        0.7298481 , 0.75847581, 0.72277561, 0.7054042 , 0.7135516 ,\n",
       "        0.72602891, 0.75498326, 0.71231836, 0.70306896, 0.71019509,\n",
       "        0.7855889 , 0.7855889 , 0.7855889 , 0.7855889 , 0.7855889 ,\n",
       "        0.77126466, 0.77126466, 0.77126466, 0.77126466, 0.77126466,\n",
       "        0.75859478, 0.75859478, 0.75859478, 0.75859478, 0.75859478,\n",
       "        0.756027  , 0.756027  , 0.756027  , 0.756027  , 0.756027  ,\n",
       "        0.79701047, 0.7681465 , 0.79701047, 0.79701047, 0.79701047,\n",
       "        0.77947541, 0.75727482, 0.77947541, 0.77947541, 0.77947541,\n",
       "        0.77353993, 0.75184521, 0.77353993, 0.77353993, 0.77353993,\n",
       "        0.77196582, 0.74655842, 0.77196582, 0.77196582, 0.77196582,\n",
       "        0.7630266 , 0.7635411 , 0.76070981, 0.7630266 , 0.7630266 ,\n",
       "        0.74751254, 0.75294291, 0.75711268, 0.74751254, 0.74751254,\n",
       "        0.73536034, 0.74761539, 0.75374363, 0.73536034, 0.73536034,\n",
       "        0.73474074, 0.74269349, 0.74565038, 0.73474074, 0.73474074,\n",
       "        0.76025876, 0.7745552 , 0.75976116, 0.76025876, 0.76025876,\n",
       "        0.73312214, 0.76614752, 0.74259834, 0.73312214, 0.73312214,\n",
       "        0.7266256 , 0.753744  , 0.73073988, 0.7266256 , 0.7266256 ,\n",
       "        0.71797415, 0.75146974, 0.72333496, 0.71797415, 0.71797415,\n",
       "        0.7854035 , 0.81425568, 0.79240569, 0.76959432, 0.78244   ,\n",
       "        0.7725973 , 0.79726176, 0.77250076, 0.76062079, 0.77162753,\n",
       "        0.76377591, 0.79288128, 0.76488258, 0.75487741, 0.7631096 ,\n",
       "        0.75897827, 0.78356454, 0.76004403, 0.74932862, 0.75797309,\n",
       "        0.81273602, 0.81273602, 0.81273602, 0.81273602, 0.81273602,\n",
       "        0.80276247, 0.80276247, 0.80276247, 0.80276247, 0.80276247,\n",
       "        0.80216007, 0.80216007, 0.80216007, 0.80216007, 0.80216007,\n",
       "        0.79398715, 0.79398715, 0.79398715, 0.79398715, 0.79398715,\n",
       "        0.8145873 , 0.80341571, 0.8145873 , 0.8145873 , 0.8145873 ,\n",
       "        0.81178493, 0.7910341 , 0.81178493, 0.81178493, 0.81178493,\n",
       "        0.80052599, 0.78994566, 0.80052599, 0.80052599, 0.80052599,\n",
       "        0.79650349, 0.78123894, 0.79650349, 0.79650349, 0.79650349,\n",
       "        0.7911326 , 0.81373295, 0.79860128, 0.7911326 , 0.7911326 ,\n",
       "        0.78620175, 0.80189217, 0.78779739, 0.78620175, 0.78620175,\n",
       "        0.78225045, 0.79192557, 0.7787349 , 0.78225045, 0.78225045,\n",
       "        0.77724617, 0.79069136, 0.77096136, 0.77724617, 0.77724617,\n",
       "        0.79398769, 0.81425568, 0.79461148, 0.79398769, 0.79398769,\n",
       "        0.78136839, 0.80190281, 0.78377449, 0.78136839, 0.78136839,\n",
       "        0.77475107, 0.7974083 , 0.77500492, 0.77475107, 0.77475107,\n",
       "        0.76489418, 0.7890595 , 0.77145266, 0.76489418, 0.76489418,\n",
       "        0.8167973 , 0.83536377, 0.82684541, 0.81678274, 0.82157178,\n",
       "        0.80558551, 0.82481587, 0.81564323, 0.80440581, 0.80637538,\n",
       "        0.79728641, 0.81956533, 0.81072591, 0.80035116, 0.79798658,\n",
       "        0.79289839, 0.8182503 , 0.80501611, 0.79678584, 0.79174711,\n",
       "        0.83941111, 0.83941111, 0.83941111, 0.83941111, 0.83941111,\n",
       "        0.83017013, 0.83017013, 0.83017013, 0.83017013, 0.83017013,\n",
       "        0.823303  , 0.823303  , 0.823303  , 0.823303  , 0.823303  ,\n",
       "        0.81768041, 0.81768041, 0.81768041, 0.81768041, 0.81768041,\n",
       "        0.83383491, 0.83661828, 0.83383491, 0.83383491, 0.83383491,\n",
       "        0.82601565, 0.82613139, 0.82601565, 0.82601565, 0.82601565,\n",
       "        0.8225566 , 0.82332455, 0.8225566 , 0.8225566 , 0.8225566 ,\n",
       "        0.81988567, 0.81945215, 0.81988567, 0.81988567, 0.81988567,\n",
       "        0.83049756, 0.83625317, 0.82620538, 0.83049756, 0.83049756,\n",
       "        0.82243791, 0.82498519, 0.82037916, 0.82243791, 0.82243791,\n",
       "        0.81765225, 0.82027463, 0.81727139, 0.81765225, 0.81765225,\n",
       "        0.81335612, 0.81916489, 0.81380286, 0.81335612, 0.81335612,\n",
       "        0.82794688, 0.83514554, 0.8259626 , 0.82794688, 0.82794688,\n",
       "        0.82126834, 0.82288735, 0.8197202 , 0.82126834, 0.82126834,\n",
       "        0.81730797, 0.81980088, 0.81574767, 0.81730797, 0.81730797,\n",
       "        0.81238202, 0.81736473, 0.81193485, 0.81238202, 0.81238202,\n",
       "        0.54472403, 0.53690409, 0.54219313, 0.54484881, 0.54485484,\n",
       "        0.73807075, 0.73247244, 0.73666629, 0.73815706, 0.73846559,\n",
       "        0.80245011, 0.80419446, 0.80256933, 0.80245017, 0.80265032,\n",
       "        0.82197617, 0.8296905 , 0.82520118, 0.82207216, 0.8220991 ,\n",
       "        0.53699834, 0.53699834, 0.53699834, 0.53699834, 0.53699834,\n",
       "        0.73216962, 0.73216962, 0.73216962, 0.73216962, 0.73216962,\n",
       "        0.80333555, 0.80333555, 0.80333555, 0.80333555, 0.80333555,\n",
       "        0.82924482, 0.82924482, 0.82924482, 0.82924482, 0.82924482,\n",
       "        0.54281494, 0.53690409, 0.54281494, 0.54281494, 0.54281494,\n",
       "        0.73879875, 0.73247244, 0.73879875, 0.73879875, 0.73879875,\n",
       "        0.80764609, 0.80419446, 0.80764609, 0.80764609, 0.80764609,\n",
       "        0.83029376, 0.8295369 , 0.83029376, 0.83029376, 0.83029376,\n",
       "        0.54317723, 0.53690409, 0.54240055, 0.54317723, 0.54317723,\n",
       "        0.73776553, 0.73247244, 0.73691047, 0.73776553, 0.73776553,\n",
       "        0.8035734 , 0.80419446, 0.8032795 , 0.8035734 , 0.8035734 ,\n",
       "        0.82465367, 0.8296905 , 0.82508224, 0.82465367, 0.82465367,\n",
       "        0.54431889, 0.53690409, 0.54219313, 0.54431889, 0.54431889,\n",
       "        0.73811564, 0.73247244, 0.73693162, 0.73811564, 0.73811564,\n",
       "        0.80320485, 0.80419446, 0.80262933, 0.80320485, 0.80320485,\n",
       "        0.82353199, 0.8296905 , 0.82544643, 0.82353199, 0.82353199,\n",
       "        0.80352972, 0.8053366 , 0.80383234, 0.8034557 , 0.80388052,\n",
       "        0.82609295, 0.83976915, 0.83060798, 0.82526481, 0.82549768,\n",
       "        0.82023932, 0.83804811, 0.82758037, 0.81883511, 0.81775877,\n",
       "        0.8155281 , 0.8340784 , 0.82276408, 0.81500658, 0.81302875,\n",
       "        0.80453402, 0.80453402, 0.80453402, 0.80453402, 0.80453402,\n",
       "        0.84232968, 0.84232968, 0.84232968, 0.84232968, 0.84232968,\n",
       "        0.84188713, 0.84188713, 0.84188713, 0.84188713, 0.84188713,\n",
       "        0.83877452, 0.83877452, 0.83877452, 0.83877452, 0.83877452,\n",
       "        0.80875407, 0.8053366 , 0.80875407, 0.80875407, 0.80875407,\n",
       "        0.83679554, 0.83987511, 0.83679554, 0.83679554, 0.83679554,\n",
       "        0.83473807, 0.83796519, 0.83473807, 0.83473807, 0.83473807,\n",
       "        0.83128053, 0.83492553, 0.83128053, 0.83128053, 0.83128053,\n",
       "        0.80448022, 0.8053366 , 0.80493541, 0.80448022, 0.80448022,\n",
       "        0.83142866, 0.83969235, 0.83255247, 0.83142866, 0.83142866,\n",
       "        0.82892763, 0.83813587, 0.83054296, 0.82892763, 0.82892763,\n",
       "        0.82565216, 0.833779  , 0.82731049, 0.82565216, 0.82565216,\n",
       "        0.80400368, 0.8053366 , 0.8038468 , 0.80400368, 0.80400368,\n",
       "        0.82834058, 0.83976915, 0.83201535, 0.82834058, 0.82834058,\n",
       "        0.82652274, 0.838169  , 0.82833978, 0.82652274, 0.82652274,\n",
       "        0.82350604, 0.83418148, 0.82455889, 0.82350604, 0.82350604,\n",
       "        0.82615745, 0.83683752, 0.83058853, 0.82629958, 0.82598579,\n",
       "        0.81793647, 0.83603018, 0.82469611, 0.81782653, 0.81804032,\n",
       "        0.81051928, 0.82985717, 0.81882473, 0.81076793, 0.81050542,\n",
       "        0.80492307, 0.82539357, 0.8143393 , 0.80432392, 0.80363997,\n",
       "        0.83862177, 0.83862177, 0.83862177, 0.83862177, 0.83862177,\n",
       "        0.84011159, 0.84011159, 0.84011159, 0.84011159, 0.84011159,\n",
       "        0.83470596, 0.83470596, 0.83470596, 0.83470596, 0.83470596,\n",
       "        0.8311833 , 0.8311833 , 0.8311833 , 0.8311833 , 0.8311833 ,\n",
       "        0.83657963, 0.8372611 , 0.83657963, 0.83657963, 0.83657963,\n",
       "        0.83425947, 0.83630984, 0.83425947, 0.83425947, 0.83425947,\n",
       "        0.82796987, 0.83061829, 0.82796987, 0.82796987, 0.82796987,\n",
       "        0.82507397, 0.8288995 , 0.82507397, 0.82507397, 0.82507397,\n",
       "        0.83151081, 0.83683752, 0.83174179, 0.83151081, 0.83151081,\n",
       "        0.828095  , 0.83634894, 0.82896926, 0.828095  , 0.828095  ,\n",
       "        0.82322498, 0.83015926, 0.8233759 , 0.82322498, 0.82322498,\n",
       "        0.82239678, 0.82629863, 0.82009385, 0.82239678, 0.82239678,\n",
       "        0.82918441, 0.83683752, 0.83086517, 0.82918441, 0.82918441,\n",
       "        0.82590935, 0.83603018, 0.82554838, 0.82590935, 0.82590935,\n",
       "        0.82149919, 0.8293729 , 0.82048702, 0.82149919, 0.82149919,\n",
       "        0.8193017 , 0.82590516, 0.81809929, 0.8193017 , 0.8193017 ]),\n",
       " 'split3_test_score': array([0.75090823, 0.80321639, 0.78470967, 0.76949181, 0.75034624,\n",
       "        0.73196452, 0.79811557, 0.75969773, 0.75540085, 0.728543  ,\n",
       "        0.72344976, 0.79662276, 0.75282652, 0.7492917 , 0.72293697,\n",
       "        0.71978285, 0.78866319, 0.74573139, 0.74691953, 0.71765242,\n",
       "        0.8211231 , 0.8211231 , 0.8211231 , 0.8211231 , 0.8211231 ,\n",
       "        0.81259747, 0.81259747, 0.81259747, 0.81259747, 0.81259747,\n",
       "        0.79126172, 0.79126172, 0.79126172, 0.79126172, 0.79126172,\n",
       "        0.79265157, 0.79265157, 0.79265157, 0.79265157, 0.79265157,\n",
       "        0.81502738, 0.8073086 , 0.81502738, 0.81502738, 0.81502738,\n",
       "        0.79460781, 0.80363716, 0.79460781, 0.79460781, 0.79460781,\n",
       "        0.78372884, 0.79425653, 0.78372884, 0.78372884, 0.78372884,\n",
       "        0.77174497, 0.78708503, 0.77174497, 0.77174497, 0.77174497,\n",
       "        0.80753798, 0.81909362, 0.79528677, 0.80753798, 0.80753798,\n",
       "        0.78784572, 0.81047503, 0.77108903, 0.78784572, 0.78784572,\n",
       "        0.7731587 , 0.79149542, 0.75855161, 0.7731587 , 0.7731587 ,\n",
       "        0.76191951, 0.78023272, 0.74844071, 0.76191951, 0.76191951,\n",
       "        0.7797747 , 0.81935785, 0.79827956, 0.7797747 , 0.7797747 ,\n",
       "        0.75321047, 0.80499939, 0.77688792, 0.75321047, 0.75321047,\n",
       "        0.74493461, 0.79803521, 0.76929753, 0.74493461, 0.74493461,\n",
       "        0.7380978 , 0.78001872, 0.76178917, 0.7380978 , 0.7380978 ,\n",
       "        0.79832742, 0.84623776, 0.83436415, 0.800154  , 0.79935111,\n",
       "        0.7782901 , 0.83528263, 0.81007685, 0.78396852, 0.78070129,\n",
       "        0.76778389, 0.82775989, 0.79446891, 0.76645111, 0.76981725,\n",
       "        0.76147629, 0.82374389, 0.78570294, 0.76196717, 0.7618018 ,\n",
       "        0.85637837, 0.85637837, 0.85637837, 0.85637837, 0.85637837,\n",
       "        0.84844198, 0.84844198, 0.84844198, 0.84844198, 0.84844198,\n",
       "        0.84557011, 0.84557011, 0.84557011, 0.84557011, 0.84557011,\n",
       "        0.84058764, 0.84058764, 0.84058764, 0.84058764, 0.84058764,\n",
       "        0.83259414, 0.8435693 , 0.83259414, 0.83259414, 0.83259414,\n",
       "        0.82661901, 0.83476373, 0.82661901, 0.82661901, 0.82661901,\n",
       "        0.81738754, 0.82894269, 0.81738754, 0.81738754, 0.81738754,\n",
       "        0.80974254, 0.82310885, 0.80974254, 0.80974254, 0.80974254,\n",
       "        0.82777156, 0.84918209, 0.84315116, 0.82777156, 0.82777156,\n",
       "        0.81308048, 0.84386978, 0.83034845, 0.81308048, 0.81308048,\n",
       "        0.80239567, 0.83883456, 0.81807588, 0.80239567, 0.80239567,\n",
       "        0.78941538, 0.83282887, 0.80763233, 0.78941538, 0.78941538,\n",
       "        0.8379089 , 0.84311678, 0.83832502, 0.8379089 , 0.8379089 ,\n",
       "        0.8141057 , 0.8375331 , 0.82592033, 0.8141057 , 0.8141057 ,\n",
       "        0.79632959, 0.82933296, 0.81383878, 0.79632959, 0.79632959,\n",
       "        0.78847743, 0.82155849, 0.80343992, 0.78847743, 0.78847743,\n",
       "        0.85226101, 0.86356323, 0.85993259, 0.85596951, 0.85288796,\n",
       "        0.84114828, 0.85977275, 0.8522418 , 0.84531384, 0.84353435,\n",
       "        0.83046914, 0.85677449, 0.84589153, 0.83550623, 0.83372092,\n",
       "        0.82205688, 0.85354328, 0.84246134, 0.82905748, 0.82616357,\n",
       "        0.86294145, 0.86294145, 0.86294145, 0.86294145, 0.86294145,\n",
       "        0.86051907, 0.86051907, 0.86051907, 0.86051907, 0.86051907,\n",
       "        0.8548292 , 0.8548292 , 0.8548292 , 0.8548292 , 0.8548292 ,\n",
       "        0.85448751, 0.85448751, 0.85448751, 0.85448751, 0.85448751,\n",
       "        0.86238881, 0.86358752, 0.86238881, 0.86238881, 0.86238881,\n",
       "        0.85867118, 0.86122016, 0.85867118, 0.85867118, 0.85867118,\n",
       "        0.85604459, 0.8588171 , 0.85604459, 0.85604459, 0.85604459,\n",
       "        0.85261813, 0.85614196, 0.85261813, 0.85261813, 0.85261813,\n",
       "        0.86160412, 0.86384901, 0.86262548, 0.86160412, 0.86160412,\n",
       "        0.85679643, 0.86266848, 0.8601175 , 0.85679643, 0.85679643,\n",
       "        0.85199711, 0.85893328, 0.85485375, 0.85199711, 0.85199711,\n",
       "        0.84843437, 0.85632442, 0.85042979, 0.84843437, 0.84843437,\n",
       "        0.85771808, 0.86356323, 0.8621179 , 0.85771808, 0.85771808,\n",
       "        0.85286241, 0.85932999, 0.85801094, 0.85286241, 0.85286241,\n",
       "        0.84740761, 0.85578845, 0.85199152, 0.84740761, 0.84740761,\n",
       "        0.84306664, 0.8520533 , 0.84604312, 0.84306664, 0.84306664,\n",
       "        0.53085881, 0.52526117, 0.5306407 , 0.53084953, 0.5309078 ,\n",
       "        0.72893714, 0.72500339, 0.73179706, 0.72887929, 0.7286136 ,\n",
       "        0.80788529, 0.80507688, 0.81106429, 0.80728795, 0.80670426,\n",
       "        0.8396109 , 0.83853407, 0.84182282, 0.83849545, 0.83823145,\n",
       "        0.52541026, 0.52541026, 0.52541026, 0.52541026, 0.52541026,\n",
       "        0.72554876, 0.72554876, 0.72554876, 0.72554876, 0.72554876,\n",
       "        0.80571669, 0.80571669, 0.80571669, 0.80571669, 0.80571669,\n",
       "        0.83920774, 0.83920774, 0.83920774, 0.83920774, 0.83920774,\n",
       "        0.53032591, 0.52526117, 0.53032591, 0.53032591, 0.53032591,\n",
       "        0.73147443, 0.72500339, 0.73147443, 0.73147443, 0.73147443,\n",
       "        0.81101199, 0.80507688, 0.81101199, 0.81101199, 0.81101199,\n",
       "        0.84246384, 0.83854876, 0.84246384, 0.84246384, 0.84246384,\n",
       "        0.53072773, 0.52526117, 0.53096976, 0.53072773, 0.53072773,\n",
       "        0.73061399, 0.72500339, 0.73172089, 0.73061399, 0.73061399,\n",
       "        0.80961258, 0.80507688, 0.81121092, 0.80961258, 0.80961258,\n",
       "        0.84152893, 0.83853407, 0.84178083, 0.84152893, 0.84152893,\n",
       "        0.53150047, 0.52526117, 0.5306407 , 0.53150047, 0.53150047,\n",
       "        0.72992589, 0.72500339, 0.73175929, 0.72992589, 0.72992589,\n",
       "        0.80857329, 0.80507688, 0.8110341 , 0.80857329, 0.80857329,\n",
       "        0.8402404 , 0.83853407, 0.8419532 , 0.8402404 , 0.8402404 ,\n",
       "        0.8094175 , 0.80661192, 0.81220388, 0.80895156, 0.80817087,\n",
       "        0.85680919, 0.86002838, 0.85795752, 0.85519272, 0.85463459,\n",
       "        0.85612927, 0.86361314, 0.85925661, 0.8547662 , 0.85557816,\n",
       "        0.8537556 , 0.86276584, 0.85877743, 0.85178068, 0.85224179,\n",
       "        0.80698585, 0.80698585, 0.80698585, 0.80698585, 0.80698585,\n",
       "        0.85969501, 0.85969501, 0.85969501, 0.85969501, 0.85969501,\n",
       "        0.8633915 , 0.8633915 , 0.8633915 , 0.8633915 , 0.8633915 ,\n",
       "        0.86318525, 0.86318525, 0.86318525, 0.86318525, 0.86318525,\n",
       "        0.81265608, 0.80661192, 0.81265608, 0.81265608, 0.81265608,\n",
       "        0.85957101, 0.86043776, 0.85957101, 0.85957101, 0.85957101,\n",
       "        0.86180831, 0.86398402, 0.86180831, 0.86180831, 0.86180831,\n",
       "        0.86035026, 0.86385784, 0.86035026, 0.86035026, 0.86035026,\n",
       "        0.81120139, 0.80661192, 0.81304802, 0.81120139, 0.81120139,\n",
       "        0.85827973, 0.86025053, 0.85945303, 0.85827973, 0.85827973,\n",
       "        0.86200565, 0.86374203, 0.86246836, 0.86200565, 0.86200565,\n",
       "        0.86270054, 0.86299897, 0.86276974, 0.86270054, 0.86270054,\n",
       "        0.81016141, 0.80661192, 0.81214838, 0.81016141, 0.81016141,\n",
       "        0.85647406, 0.86002838, 0.85864013, 0.85647406, 0.85647406,\n",
       "        0.85828095, 0.86347857, 0.86054071, 0.85828095, 0.85828095,\n",
       "        0.85891632, 0.86363131, 0.86020884, 0.85891632, 0.85891632,\n",
       "        0.85187037, 0.85412162, 0.853548  , 0.85243776, 0.85112899,\n",
       "        0.85536397, 0.86366217, 0.85894694, 0.85542562, 0.85375181,\n",
       "        0.85130869, 0.86163983, 0.85394698, 0.84715921, 0.84882829,\n",
       "        0.8481659 , 0.86005659, 0.85155284, 0.84429126, 0.8441368 ,\n",
       "        0.85360699, 0.85360699, 0.85360699, 0.85360699, 0.85360699,\n",
       "        0.86345612, 0.86345612, 0.86345612, 0.86345612, 0.86345612,\n",
       "        0.86209528, 0.86209528, 0.86209528, 0.86209528, 0.86209528,\n",
       "        0.86074731, 0.86074731, 0.86074731, 0.86074731, 0.86074731,\n",
       "        0.85477677, 0.85399905, 0.85477677, 0.85477677, 0.85477677,\n",
       "        0.86068606, 0.86396657, 0.86068606, 0.86068606, 0.86068606,\n",
       "        0.85997912, 0.86332647, 0.85997912, 0.85997912, 0.85997912,\n",
       "        0.85778098, 0.8619483 , 0.85778098, 0.85778098, 0.85778098,\n",
       "        0.85331677, 0.85412162, 0.85504893, 0.85331677, 0.85331677,\n",
       "        0.8609739 , 0.86282308, 0.86265815, 0.8609739 , 0.8609739 ,\n",
       "        0.85990153, 0.86257423, 0.86211764, 0.85990153, 0.85990153,\n",
       "        0.85895645, 0.86212313, 0.86119895, 0.85895645, 0.85895645,\n",
       "        0.85203944, 0.85412162, 0.85386241, 0.85203944, 0.85203944,\n",
       "        0.85871256, 0.86382393, 0.86107563, 0.85871256, 0.85871256,\n",
       "        0.85865467, 0.8626864 , 0.85952045, 0.85865467, 0.85865467,\n",
       "        0.85632611, 0.86207097, 0.85785396, 0.85632611, 0.85632611]),\n",
       " 'split4_test_score': array([0.68430078, 0.73386102, 0.71536509, 0.69850408, 0.70412269,\n",
       "        0.67073771, 0.71752531, 0.69620327, 0.67871546, 0.69156987,\n",
       "        0.66494858, 0.70555908, 0.68265797, 0.67254349, 0.68695063,\n",
       "        0.66272887, 0.69534705, 0.67651276, 0.67041803, 0.68442567,\n",
       "        0.76359146, 0.76359146, 0.76359146, 0.76359146, 0.76359146,\n",
       "        0.73694853, 0.73694853, 0.73694853, 0.73694853, 0.73694853,\n",
       "        0.72339405, 0.72339405, 0.72339405, 0.72339405, 0.72339405,\n",
       "        0.71412996, 0.71412996, 0.71412996, 0.71412996, 0.71412996,\n",
       "        0.73128771, 0.73557293, 0.73128771, 0.73128771, 0.73128771,\n",
       "        0.70642382, 0.72249993, 0.70642382, 0.70642382, 0.70642382,\n",
       "        0.69611212, 0.71752312, 0.69611212, 0.69611212, 0.69611212,\n",
       "        0.68927076, 0.7123254 , 0.68927076, 0.68927076, 0.68927076,\n",
       "        0.71696608, 0.73786864, 0.73080724, 0.71696608, 0.71696608,\n",
       "        0.70095698, 0.72047347, 0.70379853, 0.70095698, 0.70095698,\n",
       "        0.69711967, 0.70598439, 0.69637145, 0.69711967, 0.69711967,\n",
       "        0.68596343, 0.69085772, 0.68130429, 0.68596343, 0.68596343,\n",
       "        0.7051206 , 0.73935815, 0.71494867, 0.7051206 , 0.7051206 ,\n",
       "        0.68003822, 0.72417166, 0.69648835, 0.68003822, 0.68003822,\n",
       "        0.67235321, 0.70528527, 0.67638214, 0.67235321, 0.67235321,\n",
       "        0.66436253, 0.69912294, 0.67282758, 0.66436253, 0.66436253,\n",
       "        0.74291274, 0.77287488, 0.76182132, 0.73548897, 0.73736503,\n",
       "        0.7212731 , 0.75833473, 0.73815494, 0.71406459, 0.71128263,\n",
       "        0.70902285, 0.74750292, 0.7258998 , 0.70516607, 0.70297624,\n",
       "        0.7040857 , 0.74542594, 0.71737227, 0.69859189, 0.69413453,\n",
       "        0.78379558, 0.78379558, 0.78379558, 0.78379558, 0.78379558,\n",
       "        0.76960674, 0.76960674, 0.76960674, 0.76960674, 0.76960674,\n",
       "        0.76114519, 0.76114519, 0.76114519, 0.76114519, 0.76114519,\n",
       "        0.75375913, 0.75375913, 0.75375913, 0.75375913, 0.75375913,\n",
       "        0.77565037, 0.77622273, 0.77565037, 0.77565037, 0.77565037,\n",
       "        0.75641329, 0.75751453, 0.75641329, 0.75641329, 0.75641329,\n",
       "        0.74587209, 0.74963433, 0.74587209, 0.74587209, 0.74587209,\n",
       "        0.73854972, 0.74457277, 0.73854972, 0.73854972, 0.73854972,\n",
       "        0.7658948 , 0.77507334, 0.76339889, 0.7658948 , 0.7658948 ,\n",
       "        0.75096138, 0.75706898, 0.74751968, 0.75096138, 0.75096138,\n",
       "        0.74090571, 0.74142612, 0.73777306, 0.74090571, 0.74090571,\n",
       "        0.72994406, 0.73207676, 0.7295312 , 0.72994406, 0.72994406,\n",
       "        0.7590064 , 0.77096451, 0.75928568, 0.7590064 , 0.7590064 ,\n",
       "        0.74057845, 0.76057714, 0.73610442, 0.74057845, 0.74057845,\n",
       "        0.72591899, 0.74957164, 0.7247023 , 0.72591899, 0.72591899,\n",
       "        0.71363446, 0.74278342, 0.71816328, 0.71363446, 0.71363446,\n",
       "        0.78037908, 0.80080209, 0.78831977, 0.78192669, 0.78325347,\n",
       "        0.76743275, 0.795886  , 0.7825919 , 0.77100737, 0.77062034,\n",
       "        0.75315934, 0.79200145, 0.77456115, 0.7600271 , 0.75991804,\n",
       "        0.74658879, 0.78738978, 0.76951303, 0.75241714, 0.75087526,\n",
       "        0.80225844, 0.80225844, 0.80225844, 0.80225844, 0.80225844,\n",
       "        0.80125459, 0.80125459, 0.80125459, 0.80125459, 0.80125459,\n",
       "        0.79890574, 0.79890574, 0.79890574, 0.79890574, 0.79890574,\n",
       "        0.79636522, 0.79636522, 0.79636522, 0.79636522, 0.79636522,\n",
       "        0.79659257, 0.80121483, 0.79659257, 0.79659257, 0.79659257,\n",
       "        0.79520893, 0.79644062, 0.79520893, 0.79520893, 0.79520893,\n",
       "        0.79046311, 0.7936695 , 0.79046311, 0.79046311, 0.79046311,\n",
       "        0.78714787, 0.78842834, 0.78714787, 0.78714787, 0.78714787,\n",
       "        0.7926726 , 0.80058443, 0.7940017 , 0.7926726 , 0.7926726 ,\n",
       "        0.78568871, 0.79738275, 0.78762473, 0.78568871, 0.78568871,\n",
       "        0.78147668, 0.79313767, 0.78315725, 0.78147668, 0.78147668,\n",
       "        0.77734709, 0.78980069, 0.77845943, 0.77734709, 0.77734709,\n",
       "        0.78609853, 0.80134741, 0.79030837, 0.78609853, 0.78609853,\n",
       "        0.78000888, 0.79659882, 0.78602385, 0.78000888, 0.78000888,\n",
       "        0.77550148, 0.79159464, 0.78040531, 0.77550148, 0.77550148,\n",
       "        0.76805904, 0.78776106, 0.7763131 , 0.76805904, 0.76805904,\n",
       "        0.49190567, 0.49921067, 0.4928599 , 0.49192548, 0.49192707,\n",
       "        0.67745017, 0.68489785, 0.681736  , 0.67736936, 0.67728504,\n",
       "        0.74591307, 0.75436563, 0.75163965, 0.74581967, 0.74563873,\n",
       "        0.77106472, 0.78212697, 0.77752432, 0.7710427 , 0.77074606,\n",
       "        0.49948198, 0.49948198, 0.49948198, 0.49948198, 0.49948198,\n",
       "        0.68525558, 0.68525558, 0.68525558, 0.68525558, 0.68525558,\n",
       "        0.75411603, 0.75411603, 0.75411603, 0.75411603, 0.75411603,\n",
       "        0.77978738, 0.77978738, 0.77978738, 0.77978738, 0.77978738,\n",
       "        0.49522719, 0.49921067, 0.49522719, 0.49522719, 0.49522719,\n",
       "        0.68322895, 0.68489785, 0.68322895, 0.68322895, 0.68322895,\n",
       "        0.75244325, 0.75440931, 0.75244325, 0.75244325, 0.75244325,\n",
       "        0.77870226, 0.78216981, 0.77870226, 0.77870226, 0.77870226,\n",
       "        0.49332823, 0.49921067, 0.49370825, 0.49332823, 0.49332823,\n",
       "        0.68145482, 0.68489785, 0.68271874, 0.68145482, 0.68145482,\n",
       "        0.74981103, 0.75436563, 0.75201298, 0.74981103, 0.74981103,\n",
       "        0.77527794, 0.78212697, 0.77774474, 0.77527794, 0.77527794,\n",
       "        0.49081372, 0.49921067, 0.49290812, 0.49081372, 0.49081372,\n",
       "        0.67676734, 0.68489785, 0.68162605, 0.67676734, 0.67676734,\n",
       "        0.74573118, 0.75436563, 0.75136926, 0.74573118, 0.74573118,\n",
       "        0.7716551 , 0.78212697, 0.77742278, 0.7716551 , 0.7716551 ,\n",
       "        0.74864817, 0.75580739, 0.75301356, 0.74792873, 0.74737422,\n",
       "        0.7837899 , 0.79812857, 0.79048718, 0.78268156, 0.78118121,\n",
       "        0.78325734, 0.80009758, 0.79108869, 0.78196993, 0.78067806,\n",
       "        0.77960608, 0.79910244, 0.78628943, 0.77811669, 0.77781268,\n",
       "        0.75518365, 0.75518365, 0.75518365, 0.75518365, 0.75518365,\n",
       "        0.79764514, 0.79764514, 0.79764514, 0.79764514, 0.79764514,\n",
       "        0.80134212, 0.80134212, 0.80134212, 0.80134212, 0.80134212,\n",
       "        0.80177488, 0.80177488, 0.80177488, 0.80177488, 0.80177488,\n",
       "        0.75324732, 0.75580739, 0.75324732, 0.75324732, 0.75324732,\n",
       "        0.79367665, 0.79844693, 0.79367665, 0.79367665, 0.79367665,\n",
       "        0.79640825, 0.80105356, 0.79640825, 0.79640825, 0.79640825,\n",
       "        0.79596014, 0.80040765, 0.79596014, 0.79596014, 0.79596014,\n",
       "        0.75175948, 0.75580739, 0.75342413, 0.75175948, 0.75175948,\n",
       "        0.78974658, 0.79812857, 0.79172295, 0.78974658, 0.78974658,\n",
       "        0.79305598, 0.80050685, 0.79455188, 0.79305598, 0.79305598,\n",
       "        0.79186868, 0.7995859 , 0.79154344, 0.79186868, 0.79186868,\n",
       "        0.74716275, 0.75580739, 0.75296433, 0.74716275, 0.74716275,\n",
       "        0.78621701, 0.79812857, 0.79063456, 0.78621701, 0.78621701,\n",
       "        0.78836398, 0.80009758, 0.79263357, 0.78836398, 0.78836398,\n",
       "        0.78517335, 0.79864032, 0.78936903, 0.78517335, 0.78517335,\n",
       "        0.78091807, 0.79378819, 0.78806192, 0.78100651, 0.78162446,\n",
       "        0.78019777, 0.79980024, 0.79031493, 0.78334707, 0.7809747 ,\n",
       "        0.77305822, 0.79782789, 0.78632173, 0.77648212, 0.77338396,\n",
       "        0.76687946, 0.79701894, 0.78328361, 0.76969139, 0.76824922,\n",
       "        0.79263491, 0.79263491, 0.79263491, 0.79263491, 0.79263491,\n",
       "        0.80180961, 0.80180961, 0.80180961, 0.80180961, 0.80180961,\n",
       "        0.80184135, 0.80184135, 0.80184135, 0.80184135, 0.80184135,\n",
       "        0.799925  , 0.799925  , 0.799925  , 0.799925  , 0.799925  ,\n",
       "        0.78971478, 0.79378819, 0.78971478, 0.78971478, 0.78971478,\n",
       "        0.79690514, 0.80125145, 0.79690514, 0.79690514, 0.79690514,\n",
       "        0.79572767, 0.79931513, 0.79572767, 0.79572767, 0.79572767,\n",
       "        0.79366119, 0.79714747, 0.79366119, 0.79366119, 0.79366119,\n",
       "        0.78611848, 0.79378819, 0.78734161, 0.78611848, 0.78611848,\n",
       "        0.79363307, 0.80014667, 0.79250268, 0.79363307, 0.79363307,\n",
       "        0.79072307, 0.79783352, 0.79017299, 0.79072307, 0.79072307,\n",
       "        0.78897262, 0.79691222, 0.78870403, 0.78897262, 0.78897262,\n",
       "        0.78233311, 0.79378819, 0.78823959, 0.78233311, 0.78233311,\n",
       "        0.78726324, 0.79972436, 0.79388034, 0.78726324, 0.78726324,\n",
       "        0.78371425, 0.79813809, 0.78770753, 0.78371425, 0.78371425,\n",
       "        0.78118483, 0.79683163, 0.78599519, 0.78118483, 0.78118483]),\n",
       " 'mean_test_score': array([0.73567933, 0.78731184, 0.75523196, 0.73859786, 0.72903114,\n",
       "        0.71667381, 0.77382061, 0.73441196, 0.72167573, 0.70846952,\n",
       "        0.70779307, 0.76397466, 0.71880025, 0.71266439, 0.70160892,\n",
       "        0.70316042, 0.7544381 , 0.70972279, 0.7091689 , 0.69736762,\n",
       "        0.80388468, 0.80388468, 0.80388468, 0.80388468, 0.80388468,\n",
       "        0.78734616, 0.78734616, 0.78734616, 0.78734616, 0.78734616,\n",
       "        0.77112912, 0.77112912, 0.77112912, 0.77112912, 0.77112912,\n",
       "        0.76497318, 0.76497318, 0.76497318, 0.76497318, 0.76497318,\n",
       "        0.78811804, 0.78943726, 0.78811804, 0.78811804, 0.78811804,\n",
       "        0.76870942, 0.77538584, 0.76870942, 0.76870942, 0.76870942,\n",
       "        0.75713019, 0.76522557, 0.75713019, 0.75713019, 0.75713019,\n",
       "        0.74863373, 0.75797785, 0.74863373, 0.74863373, 0.74863373,\n",
       "        0.77241745, 0.7834063 , 0.77223194, 0.77241745, 0.77241745,\n",
       "        0.75164201, 0.76765097, 0.74965066, 0.75164201, 0.75164201,\n",
       "        0.73468807, 0.75122627, 0.7407153 , 0.73468807, 0.73468807,\n",
       "        0.7259752 , 0.74010124, 0.72977747, 0.7259752 , 0.7259752 ,\n",
       "        0.75289211, 0.79055771, 0.75862275, 0.75289211, 0.75289211,\n",
       "        0.72819255, 0.77424819, 0.73398784, 0.72819255, 0.72819255,\n",
       "        0.71815176, 0.76072322, 0.7198683 , 0.71815176, 0.71815176,\n",
       "        0.71000854, 0.75112152, 0.71063465, 0.71000854, 0.71000854,\n",
       "        0.78348497, 0.82058983, 0.80419581, 0.7775489 , 0.78462284,\n",
       "        0.76308587, 0.80521871, 0.78159476, 0.75777207, 0.76214346,\n",
       "        0.75131302, 0.79747911, 0.76754937, 0.74661858, 0.75065147,\n",
       "        0.74386268, 0.78977579, 0.75797109, 0.7391923 , 0.74161359,\n",
       "        0.82835079, 0.82835079, 0.82835079, 0.82835079, 0.82835079,\n",
       "        0.81749725, 0.81749725, 0.81749725, 0.81749725, 0.81749725,\n",
       "        0.81111242, 0.81111242, 0.81111242, 0.81111242, 0.81111242,\n",
       "        0.80376112, 0.80376112, 0.80376112, 0.80376112, 0.80376112,\n",
       "        0.8193613 , 0.82295764, 0.8193613 , 0.8193613 , 0.8193613 ,\n",
       "        0.80781753, 0.80801754, 0.80781753, 0.80781753, 0.80781753,\n",
       "        0.79629198, 0.80158506, 0.79629198, 0.79629198, 0.79629198,\n",
       "        0.78863381, 0.79284716, 0.78863381, 0.78863381, 0.78863381,\n",
       "        0.81016434, 0.82352913, 0.81087106, 0.81016434, 0.81016434,\n",
       "        0.79478839, 0.81067959, 0.79534205, 0.79478839, 0.79478839,\n",
       "        0.78386269, 0.79900999, 0.78272413, 0.78386269, 0.78386269,\n",
       "        0.77410123, 0.79126147, 0.77263178, 0.77410123, 0.77410123,\n",
       "        0.80808957, 0.82154326, 0.80749512, 0.80808957, 0.80808957,\n",
       "        0.78797318, 0.80841132, 0.79048903, 0.78797318, 0.78797318,\n",
       "        0.77360687, 0.79938236, 0.77929135, 0.77360687, 0.77360687,\n",
       "        0.76326537, 0.79077705, 0.77046925, 0.76326537, 0.76326537,\n",
       "        0.83073164, 0.84480036, 0.8387133 , 0.83128739, 0.83206891,\n",
       "        0.8171952 , 0.83921802, 0.82920658, 0.81815935, 0.81945579,\n",
       "        0.8063733 , 0.834981  , 0.82291278, 0.80900343, 0.80906696,\n",
       "        0.79908557, 0.83173396, 0.81796799, 0.80207801, 0.80134931,\n",
       "        0.84648926, 0.84648926, 0.84648926, 0.84648926, 0.84648926,\n",
       "        0.84273067, 0.84273067, 0.84273067, 0.84273067, 0.84273067,\n",
       "        0.8385811 , 0.8385811 , 0.8385811 , 0.8385811 , 0.8385811 ,\n",
       "        0.835295  , 0.835295  , 0.835295  , 0.835295  , 0.835295  ,\n",
       "        0.84312528, 0.84563654, 0.84312528, 0.84312528, 0.84312528,\n",
       "        0.83817353, 0.84056309, 0.83817353, 0.83817353, 0.83817353,\n",
       "        0.83455432, 0.83739891, 0.83455432, 0.83455432, 0.83455432,\n",
       "        0.83119083, 0.83359921, 0.83119083, 0.83119083, 0.83119083,\n",
       "        0.84030163, 0.84529676, 0.84066557, 0.84030163, 0.84030163,\n",
       "        0.83402933, 0.84066184, 0.83531417, 0.83402933, 0.83402933,\n",
       "        0.8294081 , 0.83654707, 0.83126523, 0.8294081 , 0.8294081 ,\n",
       "        0.82487925, 0.83386497, 0.8263716 , 0.82487925, 0.82487925,\n",
       "        0.8379088 , 0.84477717, 0.839643  , 0.8379088 , 0.8379088 ,\n",
       "        0.83138804, 0.83842779, 0.83352899, 0.83138804, 0.83138804,\n",
       "        0.82588372, 0.83420145, 0.82812045, 0.82588372, 0.82588372,\n",
       "        0.82065847, 0.83077048, 0.82279824, 0.82065847, 0.82065847,\n",
       "        0.53123612, 0.52705871, 0.52991434, 0.53125698, 0.53127796,\n",
       "        0.727424  , 0.72476975, 0.72791681, 0.72737397, 0.72738969,\n",
       "        0.79965672, 0.79954943, 0.80104473, 0.79955382, 0.79945849,\n",
       "        0.82546762, 0.8285456 , 0.82798267, 0.825342  , 0.8253357 ,\n",
       "        0.52713314, 0.52713314, 0.52713314, 0.52713314, 0.52713314,\n",
       "        0.72498823, 0.72498823, 0.72498823, 0.72498823, 0.72498823,\n",
       "        0.7998069 , 0.7998069 , 0.7998069 , 0.7998069 , 0.7998069 ,\n",
       "        0.82823979, 0.82823979, 0.82823979, 0.82823979, 0.82823979,\n",
       "        0.53025696, 0.52705871, 0.53025696, 0.53025696, 0.53025696,\n",
       "        0.72845112, 0.72476975, 0.72845112, 0.72845112, 0.72845112,\n",
       "        0.80202343, 0.79955817, 0.80202343, 0.80202343, 0.80202343,\n",
       "        0.82914751, 0.82852638, 0.82914751, 0.82914751, 0.82914751,\n",
       "        0.530591  , 0.52705871, 0.53016106, 0.530591  , 0.530591  ,\n",
       "        0.72783183, 0.72476975, 0.72788393, 0.72783183, 0.72783183,\n",
       "        0.80007164, 0.79954943, 0.8008339 , 0.80007164, 0.80007164,\n",
       "        0.82669024, 0.8285456 , 0.82770659, 0.82669024, 0.82669024,\n",
       "        0.53081036, 0.52705871, 0.52989054, 0.53081035, 0.53081035,\n",
       "        0.72727427, 0.72476975, 0.72778996, 0.72727447, 0.72727447,\n",
       "        0.79974501, 0.79954943, 0.80089227, 0.79974522, 0.79974522,\n",
       "        0.82612468, 0.8285456 , 0.8281261 , 0.82612491, 0.82612491,\n",
       "        0.8010868 , 0.80094494, 0.8021047 , 0.800832  , 0.80068525,\n",
       "        0.83633464, 0.8440319 , 0.8401391 , 0.83559079, 0.83533039,\n",
       "        0.83337553, 0.8458311 , 0.84010106, 0.83256763, 0.83280295,\n",
       "        0.82923428, 0.84377625, 0.8370335 , 0.82825131, 0.82799864,\n",
       "        0.80096887, 0.80096887, 0.80096887, 0.80096887, 0.80096887,\n",
       "        0.84451819, 0.84451819, 0.84451819, 0.84451819, 0.84451819,\n",
       "        0.8471758 , 0.8471758 , 0.8471758 , 0.8471758 , 0.8471758 ,\n",
       "        0.84627823, 0.84627823, 0.84627823, 0.84627823, 0.84627823,\n",
       "        0.80323777, 0.80094494, 0.80323777, 0.80323777, 0.80323777,\n",
       "        0.8422966 , 0.84422132, 0.8422966 , 0.8422966 , 0.8422966 ,\n",
       "        0.84303165, 0.84608251, 0.84303165, 0.84303165, 0.84303165,\n",
       "        0.84169967, 0.84469508, 0.84169967, 0.84169967, 0.84169967,\n",
       "        0.8014344 , 0.80094494, 0.80221212, 0.8014344 , 0.8014344 ,\n",
       "        0.83926776, 0.84406097, 0.84089026, 0.83926776, 0.83926776,\n",
       "        0.84031144, 0.84586193, 0.84196284, 0.84031144, 0.84031144,\n",
       "        0.83890525, 0.84416221, 0.8405222 , 0.83890525, 0.83890525,\n",
       "        0.80101275, 0.80094494, 0.80216362, 0.80101275, 0.80101275,\n",
       "        0.83824676, 0.8440319 , 0.84073656, 0.83824676, 0.83824676,\n",
       "        0.83867287, 0.84576024, 0.84104901, 0.83867287, 0.83867287,\n",
       "        0.83683053, 0.843962  , 0.83883804, 0.83683053, 0.83683053,\n",
       "        0.83434774, 0.83984594, 0.83735087, 0.83446879, 0.83449672,\n",
       "        0.83153347, 0.84521206, 0.83812755, 0.83206834, 0.83118221,\n",
       "        0.8249684 , 0.84228399, 0.83302455, 0.82423149, 0.82357871,\n",
       "        0.8192472 , 0.84005684, 0.82901538, 0.81831047, 0.81717815,\n",
       "        0.84015749, 0.84015749, 0.84015749, 0.84015749, 0.84015749,\n",
       "        0.84673277, 0.84673277, 0.84673277, 0.84673277, 0.84673277,\n",
       "        0.84486475, 0.84486475, 0.84486475, 0.84486475, 0.84486475,\n",
       "        0.8428627 , 0.8428627 , 0.8428627 , 0.8428627 , 0.8428627 ,\n",
       "        0.83935626, 0.84002163, 0.83935626, 0.83935626, 0.83935626,\n",
       "        0.84290478, 0.84553012, 0.84290478, 0.84290478, 0.84290478,\n",
       "        0.84045194, 0.84266379, 0.84045194, 0.84045194, 0.84045194,\n",
       "        0.83782828, 0.84082703, 0.83782828, 0.83782828, 0.83782828,\n",
       "        0.83652548, 0.83984594, 0.83763574, 0.83652548, 0.83652548,\n",
       "        0.84002982, 0.84526706, 0.84087216, 0.84002982, 0.84002982,\n",
       "        0.83720695, 0.84259183, 0.83829089, 0.83720695, 0.83720695,\n",
       "        0.83493579, 0.84077801, 0.83591326, 0.83493579, 0.83493579,\n",
       "        0.83560575, 0.83984594, 0.83787235, 0.83560575, 0.83560575,\n",
       "        0.83811557, 0.84512816, 0.84042236, 0.83811557, 0.83811557,\n",
       "        0.83498551, 0.84276127, 0.83657972, 0.83498551, 0.83498551,\n",
       "        0.83211708, 0.8406126 , 0.83363584, 0.83211708, 0.83211708]),\n",
       " 'std_test_score': array([0.03129374, 0.03468969, 0.02969138, 0.02846033, 0.02457815,\n",
       "        0.03372638, 0.03252705, 0.02858428, 0.02929468, 0.02951911,\n",
       "        0.03646797, 0.0362859 , 0.03060527, 0.03152747, 0.02975201,\n",
       "        0.03637135, 0.03752946, 0.03197978, 0.03157639, 0.03100227,\n",
       "        0.02819256, 0.02819256, 0.02819256, 0.02819256, 0.02819256,\n",
       "        0.03244965, 0.03244965, 0.03244965, 0.03244965, 0.03244965,\n",
       "        0.03130494, 0.03130494, 0.03130494, 0.03130494, 0.03130494,\n",
       "        0.03265771, 0.03265771, 0.03265771, 0.03265771, 0.03265771,\n",
       "        0.03537208, 0.03594258, 0.03537208, 0.03537208, 0.03537208,\n",
       "        0.03613224, 0.03565367, 0.03613224, 0.03613224, 0.03613224,\n",
       "        0.03476807, 0.03496487, 0.03476807, 0.03476807, 0.03476807,\n",
       "        0.03443025, 0.03398971, 0.03443025, 0.03443025, 0.03443025,\n",
       "        0.03406782, 0.03554191, 0.02593635, 0.03406782, 0.03406782,\n",
       "        0.03570956, 0.03523353, 0.02697522, 0.03570956, 0.03570956,\n",
       "        0.03649029, 0.03347865, 0.02667207, 0.03649029, 0.03649029,\n",
       "        0.03861656, 0.03409081, 0.03019333, 0.03861656, 0.03861656,\n",
       "        0.02745735, 0.03310554, 0.03783203, 0.02745735, 0.02745735,\n",
       "        0.02623081, 0.03208175, 0.04144691, 0.02623081, 0.02623081,\n",
       "        0.02709922, 0.03578924, 0.04690142, 0.02709922, 0.02709922,\n",
       "        0.02828154, 0.03251553, 0.04933103, 0.02828154, 0.02828154,\n",
       "        0.0246902 , 0.02799459, 0.02766427, 0.02894243, 0.02864516,\n",
       "        0.02629052, 0.02927027, 0.02647815, 0.02843928, 0.03126034,\n",
       "        0.02872297, 0.02960672, 0.02641067, 0.02718928, 0.03143679,\n",
       "        0.02867738, 0.02762772, 0.0267001 , 0.02825109, 0.03266643,\n",
       "        0.02851979, 0.02851979, 0.02851979, 0.02851979, 0.02851979,\n",
       "        0.02991703, 0.02991703, 0.02991703, 0.02991703, 0.02991703,\n",
       "        0.03087602, 0.03087602, 0.03087602, 0.03087602, 0.03087602,\n",
       "        0.03182153, 0.03182153, 0.03182153, 0.03182153, 0.03182153,\n",
       "        0.02602244, 0.03022265, 0.02602244, 0.02602244, 0.02602244,\n",
       "        0.02852946, 0.03171312, 0.02852946, 0.02852946, 0.02852946,\n",
       "        0.0282126 , 0.03172674, 0.0282126 , 0.0282126 , 0.0282126 ,\n",
       "        0.02841383, 0.03034535, 0.02841383, 0.02841383, 0.02841383,\n",
       "        0.02850199, 0.02929803, 0.03079988, 0.02850199, 0.02850199,\n",
       "        0.02568988, 0.03208683, 0.0304669 , 0.02568988, 0.02568988,\n",
       "        0.02417197, 0.0347578 , 0.02935485, 0.02417197, 0.02417197,\n",
       "        0.02437479, 0.0351421 , 0.02916041, 0.02437479, 0.02437479,\n",
       "        0.03193549, 0.02933949, 0.02989873, 0.03193549, 0.03193549,\n",
       "        0.03030393, 0.03018012, 0.0331577 , 0.03030393, 0.03030393,\n",
       "        0.0290583 , 0.03095976, 0.03291587, 0.0290583 , 0.0290583 ,\n",
       "        0.0310149 , 0.03050016, 0.03112637, 0.0310149 , 0.0310149 ,\n",
       "        0.03017078, 0.02652128, 0.02982483, 0.02967467, 0.02913146,\n",
       "        0.02996158, 0.02708017, 0.02842634, 0.02923722, 0.0299964 ,\n",
       "        0.03089001, 0.02750514, 0.02903993, 0.02927575, 0.02996839,\n",
       "        0.03012935, 0.02782489, 0.02918763, 0.02936091, 0.03041576,\n",
       "        0.02605841, 0.02605841, 0.02605841, 0.02605841, 0.02605841,\n",
       "        0.02563332, 0.02563332, 0.02563332, 0.02563332, 0.02563332,\n",
       "        0.0256868 , 0.0256868 , 0.0256868 , 0.0256868 , 0.0256868 ,\n",
       "        0.02625843, 0.02625843, 0.02625843, 0.02625843, 0.02625843,\n",
       "        0.0277618 , 0.02649108, 0.0277618 , 0.0277618 , 0.0277618 ,\n",
       "        0.02648478, 0.02743951, 0.02648478, 0.02648478, 0.02648478,\n",
       "        0.0273043 , 0.02732554, 0.0273043 , 0.0273043 , 0.0273043 ,\n",
       "        0.02734703, 0.02837132, 0.02734703, 0.02734703, 0.02734703,\n",
       "        0.02849439, 0.0264603 , 0.02917303, 0.02849439, 0.02849439,\n",
       "        0.02898338, 0.02725601, 0.02960326, 0.02898338, 0.02898338,\n",
       "        0.02896762, 0.02756794, 0.02985163, 0.02896762, 0.02896762,\n",
       "        0.02918889, 0.02773685, 0.02966277, 0.02918889, 0.02918889,\n",
       "        0.03003255, 0.02604615, 0.03001887, 0.03003255, 0.03003255,\n",
       "        0.02975404, 0.02681275, 0.02902932, 0.02975404, 0.02975404,\n",
       "        0.02916787, 0.02717361, 0.02906691, 0.02916787, 0.02916787,\n",
       "        0.0302025 , 0.02743979, 0.02854696, 0.0302025 , 0.0302025 ,\n",
       "        0.02443963, 0.02342255, 0.02429408, 0.02443568, 0.02444325,\n",
       "        0.02887685, 0.02713171, 0.02789735, 0.02884098, 0.02888973,\n",
       "        0.03033891, 0.02792399, 0.02899413, 0.03027373, 0.03022222,\n",
       "        0.03083402, 0.02756619, 0.0292927 , 0.03070312, 0.03073688,\n",
       "        0.02338293, 0.02338293, 0.02338293, 0.02338293, 0.02338293,\n",
       "        0.02714328, 0.02714328, 0.02714328, 0.02714328, 0.02714328,\n",
       "        0.02811128, 0.02811128, 0.02811128, 0.02811128, 0.02811128,\n",
       "        0.02824908, 0.02824908, 0.02824908, 0.02824908, 0.02824908,\n",
       "        0.02370583, 0.02342255, 0.02370583, 0.02370583, 0.02370583,\n",
       "        0.02747209, 0.02713171, 0.02747209, 0.02747209, 0.02747209,\n",
       "        0.02858143, 0.02790986, 0.02858143, 0.02858143, 0.02858143,\n",
       "        0.0286562 , 0.02755163, 0.0286562 , 0.0286562 , 0.0286562 ,\n",
       "        0.0240063 , 0.02342255, 0.0240254 , 0.0240063 , 0.0240063 ,\n",
       "        0.02778725, 0.02713171, 0.02746528, 0.02778725, 0.02778725,\n",
       "        0.02925806, 0.02792399, 0.02863699, 0.02925806, 0.02925806,\n",
       "        0.02971919, 0.02756619, 0.02900412, 0.02971919, 0.02971919,\n",
       "        0.02461213, 0.02342255, 0.02426015, 0.02461211, 0.02461211,\n",
       "        0.02908926, 0.02713171, 0.02792187, 0.02908953, 0.02908953,\n",
       "        0.03038116, 0.02792399, 0.02909752, 0.03038145, 0.03038145,\n",
       "        0.03059048, 0.02756619, 0.02933825, 0.03059076, 0.03059076,\n",
       "        0.02967295, 0.02784167, 0.02865979, 0.02987071, 0.02995966,\n",
       "        0.0309537 , 0.02714822, 0.02977148, 0.03130729, 0.03145089,\n",
       "        0.03019379, 0.02690414, 0.02942619, 0.03043476, 0.03134417,\n",
       "        0.03023414, 0.02652817, 0.03038345, 0.0302941 , 0.03052224,\n",
       "        0.02815638, 0.02815638, 0.02815638, 0.02815638, 0.02815638,\n",
       "        0.02725201, 0.02725201, 0.02725201, 0.02725201, 0.02725201,\n",
       "        0.02666303, 0.02666303, 0.02666303, 0.02666303, 0.02666303,\n",
       "        0.02613401, 0.02613401, 0.02613401, 0.02613401, 0.02613401,\n",
       "        0.02868631, 0.02784167, 0.02868631, 0.02868631, 0.02868631,\n",
       "        0.0283317 , 0.02702963, 0.0283317 , 0.0283317 , 0.0283317 ,\n",
       "        0.02773706, 0.02683941, 0.02773706, 0.02773706, 0.02773706,\n",
       "        0.02760305, 0.02658817, 0.02760305, 0.02760305, 0.02760305,\n",
       "        0.02891549, 0.02784167, 0.02854665, 0.02891549, 0.02891549,\n",
       "        0.02948869, 0.02717698, 0.0292038 , 0.02948869, 0.02948869,\n",
       "        0.02887074, 0.02682137, 0.02864379, 0.02887074, 0.02887074,\n",
       "        0.02912716, 0.02685974, 0.02937591, 0.02912716, 0.02912716,\n",
       "        0.03028789, 0.02784167, 0.02892083, 0.03028789, 0.03028789,\n",
       "        0.03042634, 0.02714822, 0.02970479, 0.03042634, 0.03042634,\n",
       "        0.02975858, 0.02694154, 0.02914135, 0.02975858, 0.02975858,\n",
       "        0.03046268, 0.0269131 , 0.02980858, 0.03046268, 0.03046268,\n",
       "        0.03087008, 0.02725588, 0.02919439, 0.03091487, 0.03061959,\n",
       "        0.0307983 , 0.02691834, 0.02899089, 0.02978673, 0.03028829,\n",
       "        0.03127006, 0.026902  , 0.02844062, 0.02925548, 0.03063441,\n",
       "        0.03167601, 0.02691661, 0.0284169 , 0.02997828, 0.03050033,\n",
       "        0.02758729, 0.02758729, 0.02758729, 0.02758729, 0.02758729,\n",
       "        0.02635109, 0.02635109, 0.02635109, 0.02635109, 0.02635109,\n",
       "        0.02587663, 0.02587663, 0.02587663, 0.02587663, 0.02587663,\n",
       "        0.02613323, 0.02613323, 0.02613323, 0.02613323, 0.02613323,\n",
       "        0.02845786, 0.02722534, 0.02845786, 0.02845786, 0.02845786,\n",
       "        0.02733237, 0.02652861, 0.02733237, 0.02733237, 0.02733237,\n",
       "        0.02746971, 0.02663718, 0.02746971, 0.02746971, 0.02746971,\n",
       "        0.0275883 , 0.02673148, 0.0275883 , 0.0275883 , 0.0275883 ,\n",
       "        0.02940413, 0.02725588, 0.0293646 , 0.02940413, 0.02940413,\n",
       "        0.02867451, 0.02680296, 0.02918404, 0.02867451, 0.02867451,\n",
       "        0.02880786, 0.02722229, 0.02938301, 0.02880786, 0.02880786,\n",
       "        0.02841189, 0.02737815, 0.02926361, 0.02841189, 0.02841189,\n",
       "        0.03064116, 0.02725588, 0.02917215, 0.03064116, 0.03064116,\n",
       "        0.0300785 , 0.02693617, 0.02891749, 0.0300785 , 0.0300785 ,\n",
       "        0.03073348, 0.02724777, 0.03012902, 0.03073348, 0.03073348,\n",
       "        0.03047408, 0.02742344, 0.02935295, 0.03047408, 0.03047408]),\n",
       " 'rank_test_score': array([518, 436, 494, 517, 525, 563, 452, 522, 557, 571, 572, 479, 559,\n",
       "        564, 574, 573, 495, 569, 570, 575, 338, 338, 338, 338, 338, 431,\n",
       "        431, 431, 431, 431, 461, 461, 461, 461, 461, 474, 474, 474, 474,\n",
       "        474, 424, 419, 424, 424, 424, 467, 447, 467, 467, 467, 490, 473,\n",
       "        490, 490, 490, 507, 487, 507, 507, 507, 457, 442, 460, 457, 457,\n",
       "        499, 471, 506, 499, 499, 519, 503, 514, 519, 519, 545, 515, 524,\n",
       "        545, 545, 496, 416, 486, 496, 496, 530, 448, 523, 530, 530, 560,\n",
       "        485, 558, 560, 560, 566, 504, 565, 566, 566, 441, 296, 337, 446,\n",
       "        437, 483, 336, 444, 489, 484, 502, 404, 472, 511, 505, 512, 418,\n",
       "        488, 516, 513, 253, 253, 253, 253, 253, 306, 306, 306, 306, 306,\n",
       "        313, 313, 313, 313, 313, 343, 343, 343, 343, 343, 298, 289, 298,\n",
       "        298, 298, 330, 329, 330, 330, 330, 405, 360, 405, 405, 405, 420,\n",
       "        413, 420, 420, 420, 320, 288, 318, 320, 320, 410, 319, 409, 410,\n",
       "        410, 438, 403, 443, 438, 438, 449, 414, 456, 449, 449, 326, 292,\n",
       "        334, 326, 326, 428, 325, 417, 428, 428, 453, 401, 445, 453, 453,\n",
       "        480, 415, 466, 480, 480, 238,  36, 136, 230, 223, 311, 131, 243,\n",
       "        304, 297, 335, 197, 290, 324, 323, 402, 225, 305, 355, 364,  11,\n",
       "         11,  11,  11,  11,  69,  69,  69,  69,  69, 140, 140, 140, 140,\n",
       "        140, 189, 189, 189, 189, 189,  51,  25,  51,  51,  51, 150,  95,\n",
       "        150, 150, 150, 201, 167, 201, 201, 201, 232, 214, 232, 232, 232,\n",
       "        105,  27,  92, 105, 105, 209,  93, 188, 209, 209, 239, 177, 231,\n",
       "        239, 239, 283, 212, 272, 283, 283, 158,  37, 123, 158, 158, 227,\n",
       "        145, 215, 227, 227, 276, 208, 265, 276, 276, 293, 237, 291, 293,\n",
       "        293, 578, 597, 590, 577, 576, 539, 553, 533, 541, 540, 394, 397,\n",
       "        366, 396, 400, 279, 249, 267, 280, 281, 592, 592, 592, 592, 592,\n",
       "        548, 548, 548, 548, 548, 386, 386, 386, 386, 386, 259, 259, 259,\n",
       "        259, 259, 585, 597, 585, 585, 585, 526, 553, 526, 526, 526, 356,\n",
       "        395, 356, 356, 356, 244, 252, 244, 244, 244, 582, 597, 589, 582,\n",
       "        582, 535, 553, 534, 535, 535, 383, 397, 380, 383, 383, 269, 249,\n",
       "        268, 269, 269, 579, 597, 591, 580, 580, 544, 553, 538, 542, 542,\n",
       "        393, 397, 379, 391, 391, 275, 249, 264, 273, 273, 365, 375, 354,\n",
       "        381, 382, 181,  47, 113, 186, 187, 216,  23, 114, 219, 218, 242,\n",
       "         50, 172, 258, 266, 370, 370, 370, 370, 370,  39,  39,  39,  39,\n",
       "         39,   1,   1,   1,   1,   1,  16,  16,  16,  16,  16, 348, 375,\n",
       "        348, 348, 348,  76,  44,  76,  76,  76,  55,  21,  55,  55,  55,\n",
       "         82,  38,  82,  82,  82, 361, 375, 352, 361, 361, 128,  46,  87,\n",
       "        128, 128, 102,  22,  81, 102, 102, 132,  45,  96, 132, 132, 367,\n",
       "        375, 353, 367, 367, 147,  47,  91, 147, 147, 137,  24,  86, 137,\n",
       "        137, 173,  49, 135, 173, 173, 207, 120, 168, 206, 205, 226,  29,\n",
       "        154, 224, 236, 282,  80, 217, 286, 287, 302, 115, 248, 303, 312,\n",
       "        108, 108, 108, 108, 108,   6,   6,   6,   6,   6,  31,  31,  31,\n",
       "         31,  31,  63,  63,  63,  63,  63, 124, 119, 124, 124, 124,  59,\n",
       "         26,  59,  59,  59,  97,  74,  97,  97,  97, 162,  89, 162, 162,\n",
       "        162, 178, 120, 166, 178, 178, 116,  28,  88, 116, 116, 169,  75,\n",
       "        146, 169, 169, 198,  90, 182, 198, 198, 183, 120, 161, 183, 183,\n",
       "        155,  30, 101, 155, 155, 194,  68, 176, 194, 194, 220,  94, 213,\n",
       "        220, 220], dtype=int32)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result = grid_model.cv_results_\n",
    "grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f602a22-8e18-49f0-b17c-f66f2147319d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_num_leaves</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.354687</td>\n",
       "      <td>0.217181</td>\n",
       "      <td>0.011070</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>50</td>\n",
       "      <td>31</td>\n",
       "      <td>{'learning_rate': 1.0, 'max_depth': -1, 'n_est...</td>\n",
       "      <td>0.776905</td>\n",
       "      <td>0.720644</td>\n",
       "      <td>0.745639</td>\n",
       "      <td>0.750908</td>\n",
       "      <td>0.684301</td>\n",
       "      <td>0.735679</td>\n",
       "      <td>0.031294</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.074626</td>\n",
       "      <td>0.010509</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 1.0, 'max_depth': -1, 'n_est...</td>\n",
       "      <td>0.831273</td>\n",
       "      <td>0.805942</td>\n",
       "      <td>0.762267</td>\n",
       "      <td>0.803216</td>\n",
       "      <td>0.733861</td>\n",
       "      <td>0.787312</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.123285</td>\n",
       "      <td>0.014033</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>{'learning_rate': 1.0, 'max_depth': -1, 'n_est...</td>\n",
       "      <td>0.791573</td>\n",
       "      <td>0.730170</td>\n",
       "      <td>0.754342</td>\n",
       "      <td>0.784710</td>\n",
       "      <td>0.715365</td>\n",
       "      <td>0.755232</td>\n",
       "      <td>0.029691</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.184625</td>\n",
       "      <td>0.022932</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 1.0, 'max_depth': -1, 'n_est...</td>\n",
       "      <td>0.772859</td>\n",
       "      <td>0.726326</td>\n",
       "      <td>0.725808</td>\n",
       "      <td>0.769492</td>\n",
       "      <td>0.698504</td>\n",
       "      <td>0.738598</td>\n",
       "      <td>0.028460</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.213914</td>\n",
       "      <td>0.011705</td>\n",
       "      <td>0.010452</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>50</td>\n",
       "      <td>64</td>\n",
       "      <td>{'learning_rate': 1.0, 'max_depth': -1, 'n_est...</td>\n",
       "      <td>0.764385</td>\n",
       "      <td>0.703368</td>\n",
       "      <td>0.722934</td>\n",
       "      <td>0.750346</td>\n",
       "      <td>0.704123</td>\n",
       "      <td>0.729031</td>\n",
       "      <td>0.024578</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.488320</td>\n",
       "      <td>0.028412</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>31</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 6, 'n_est...</td>\n",
       "      <td>0.867969</td>\n",
       "      <td>0.835804</td>\n",
       "      <td>0.819302</td>\n",
       "      <td>0.856326</td>\n",
       "      <td>0.781185</td>\n",
       "      <td>0.832117</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.231779</td>\n",
       "      <td>0.030157</td>\n",
       "      <td>0.011106</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 6, 'n_est...</td>\n",
       "      <td>0.874637</td>\n",
       "      <td>0.843618</td>\n",
       "      <td>0.825905</td>\n",
       "      <td>0.862071</td>\n",
       "      <td>0.796832</td>\n",
       "      <td>0.840613</td>\n",
       "      <td>0.027423</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.377348</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>0.012417</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 6, 'n_est...</td>\n",
       "      <td>0.868141</td>\n",
       "      <td>0.838090</td>\n",
       "      <td>0.818099</td>\n",
       "      <td>0.857854</td>\n",
       "      <td>0.785995</td>\n",
       "      <td>0.833636</td>\n",
       "      <td>0.029353</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0.476087</td>\n",
       "      <td>0.017766</td>\n",
       "      <td>0.012842</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 6, 'n_est...</td>\n",
       "      <td>0.867969</td>\n",
       "      <td>0.835804</td>\n",
       "      <td>0.819302</td>\n",
       "      <td>0.856326</td>\n",
       "      <td>0.781185</td>\n",
       "      <td>0.832117</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.479553</td>\n",
       "      <td>0.040796</td>\n",
       "      <td>0.011722</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>64</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 6, 'n_est...</td>\n",
       "      <td>0.867969</td>\n",
       "      <td>0.835804</td>\n",
       "      <td>0.819302</td>\n",
       "      <td>0.856326</td>\n",
       "      <td>0.781185</td>\n",
       "      <td>0.832117</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         0.354687      0.217181         0.011070        0.001529   \n",
       "1         0.074626      0.010509         0.012800        0.006969   \n",
       "2         0.123285      0.014033         0.009703        0.001069   \n",
       "3         0.184625      0.022932         0.009268        0.000803   \n",
       "4         0.213914      0.011705         0.010452        0.000583   \n",
       "..             ...           ...              ...             ...   \n",
       "595       0.488320      0.028412         0.013782        0.003153   \n",
       "596       0.231779      0.030157         0.011106        0.000732   \n",
       "597       0.377348      0.015758         0.012417        0.003060   \n",
       "598       0.476087      0.017766         0.012842        0.001115   \n",
       "599       0.479553      0.040796         0.011722        0.001550   \n",
       "\n",
       "     param_learning_rate  param_max_depth  param_n_estimators  \\\n",
       "0                   1.00               -1                  50   \n",
       "1                   1.00               -1                  50   \n",
       "2                   1.00               -1                  50   \n",
       "3                   1.00               -1                  50   \n",
       "4                   1.00               -1                  50   \n",
       "..                   ...              ...                 ...   \n",
       "595                 0.05                6                 200   \n",
       "596                 0.05                6                 200   \n",
       "597                 0.05                6                 200   \n",
       "598                 0.05                6                 200   \n",
       "599                 0.05                6                 200   \n",
       "\n",
       "     param_num_leaves                                             params  \\\n",
       "0                  31  {'learning_rate': 1.0, 'max_depth': -1, 'n_est...   \n",
       "1                   8  {'learning_rate': 1.0, 'max_depth': -1, 'n_est...   \n",
       "2                  16  {'learning_rate': 1.0, 'max_depth': -1, 'n_est...   \n",
       "3                  32  {'learning_rate': 1.0, 'max_depth': -1, 'n_est...   \n",
       "4                  64  {'learning_rate': 1.0, 'max_depth': -1, 'n_est...   \n",
       "..                ...                                                ...   \n",
       "595                31  {'learning_rate': 0.05, 'max_depth': 6, 'n_est...   \n",
       "596                 8  {'learning_rate': 0.05, 'max_depth': 6, 'n_est...   \n",
       "597                16  {'learning_rate': 0.05, 'max_depth': 6, 'n_est...   \n",
       "598                32  {'learning_rate': 0.05, 'max_depth': 6, 'n_est...   \n",
       "599                64  {'learning_rate': 0.05, 'max_depth': 6, 'n_est...   \n",
       "\n",
       "     split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0             0.776905           0.720644           0.745639   \n",
       "1             0.831273           0.805942           0.762267   \n",
       "2             0.791573           0.730170           0.754342   \n",
       "3             0.772859           0.726326           0.725808   \n",
       "4             0.764385           0.703368           0.722934   \n",
       "..                 ...                ...                ...   \n",
       "595           0.867969           0.835804           0.819302   \n",
       "596           0.874637           0.843618           0.825905   \n",
       "597           0.868141           0.838090           0.818099   \n",
       "598           0.867969           0.835804           0.819302   \n",
       "599           0.867969           0.835804           0.819302   \n",
       "\n",
       "     split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0             0.750908           0.684301         0.735679        0.031294   \n",
       "1             0.803216           0.733861         0.787312        0.034690   \n",
       "2             0.784710           0.715365         0.755232        0.029691   \n",
       "3             0.769492           0.698504         0.738598        0.028460   \n",
       "4             0.750346           0.704123         0.729031        0.024578   \n",
       "..                 ...                ...              ...             ...   \n",
       "595           0.856326           0.781185         0.832117        0.030474   \n",
       "596           0.862071           0.796832         0.840613        0.027423   \n",
       "597           0.857854           0.785995         0.833636        0.029353   \n",
       "598           0.856326           0.781185         0.832117        0.030474   \n",
       "599           0.856326           0.781185         0.832117        0.030474   \n",
       "\n",
       "     rank_test_score  \n",
       "0                518  \n",
       "1                436  \n",
       "2                494  \n",
       "3                517  \n",
       "4                525  \n",
       "..               ...  \n",
       "595              220  \n",
       "596               94  \n",
       "597              213  \n",
       "598              220  \n",
       "599              220  \n",
       "\n",
       "[600 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame.from_dict(grid_result)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa95a414-df5c-4718-8f83-94846d921659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 150, 'num_leaves': 31}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6414ec9-bf83-4579-9022-a338e16b766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(learning_rate=0.03, max_depth=3, n_estimators=150)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LGBMRegressor</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(learning_rate=0.03, max_depth=3, n_estimators=150)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(learning_rate=0.03, max_depth=3, n_estimators=150)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa789781-edf4-4da9-8825-5e16fa796ac2",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36ba3ad2-3c7d-4983-9b0e-5fce3c17097d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.847175796130581)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  \n",
    "parameter                                                                       Accuracy\n",
    "  {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 150, 'num_leaves': 31}   -   0.847175796130581 \n",
    "\"\"\"\n",
    "\n",
    "grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4577b9-2713-4c8f-a005-001950b15cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "711a10b9-cd97-4d2f-88fa-9c94bf3e2745",
   "metadata": {},
   "source": [
    "## save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05699548-8401-4c3b-a0f9-8cdfea819326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# pickle.dump(regressor, open(\"lightGBM_finalmodel_insurance_charge_predict.sav\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ccdfa7-8c6f-4230-aa1d-8f802d0c12e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bc1445a-b799-4ea1-b8f3-5fe9d54afcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([12952.89103167])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.predict([[ 52, 30.200, 1, 1, 0 ]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718416d-f268-4846-ab64-9e118e010765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36c62c-db33-4086-ae60-944d15ccf976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
